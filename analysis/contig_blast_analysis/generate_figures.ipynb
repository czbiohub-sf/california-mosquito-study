{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ete3 import NCBITaxa\n",
    "import boto3\n",
    "import tempfile\n",
    "import subprocess\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "ncbi = NCBITaxa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_viral_family_df (row_x):\n",
    "    segments = row_x[\"segments\"]\n",
    "    df_by_sample = pd.DataFrame([x.split(\"|\") for x in segments[list(segments.keys())[0]][\"contigs\"]])\n",
    "    df_by_sample = df_by_sample.assign(family=row_x[\"family\"], taxid=row_x[\"taxid\"])\n",
    "    if not pd.isnull(row_x[\"provisional_name\"]):\n",
    "        df_by_sample = df_by_sample.assign(sci_name=row_x[\"provisional_name\"])\n",
    "    else:\n",
    "        df_by_sample = df_by_sample.assign(sci_name=row_x[\"name\"])\n",
    "    df_by_sample = df_by_sample.rename(columns={0:\"sample\", 1:\"contig_name\"})\n",
    "    return (df_by_sample)\n",
    "\n",
    "def get_rows_taxid (df, taxid, taxid_colname=\"taxid\", identity_qcov_cutoff=None):\n",
    "    if (not isinstance(df, pd.DataFrame)):\n",
    "        if isinstance(taxid, str):\n",
    "            taxid = ncbi.get_name_translator([taxid])[taxid][0]\n",
    "        return (taxid in ncbi.get_lineage(df))\n",
    "    outdf = df[df[taxid_colname].apply(get_rows_taxid, taxid=taxid)]\n",
    "    if identity_qcov_cutoff is not None:\n",
    "        outdf = outdf[outdf[\"identity_qcov\"]>=identity_qcov_cutoff]\n",
    "    return (outdf)\n",
    "\n",
    "\n",
    "# def filter_by_criterion (df, colname, minthreshold, bysample=True):\n",
    "#     if bysample:\n",
    "#         sums = df.groupby([\"sample\", \"taxid\"])[colname].sum().reset_index()\n",
    "#         sums[\"tokeep\"] = sums[colname] >= minthreshold\n",
    "#         df = pd.merge(df, sums.drop(columns=colname), how=\"left\")\n",
    "#         df = df[df[\"tokeep\"]!=False].drop(columns=\"tokeep\")\n",
    "#     else:\n",
    "#         df = df[df[colname] >= minthreshold]\n",
    "#     return (df)\n",
    "\n",
    "\n",
    "def check_if_in_any_taxid(taxid, taxid_list):\n",
    "    if taxid in taxid_list:\n",
    "        return (taxid)\n",
    "    taxids = ncbi.get_lineage(taxid)\n",
    "    check_in = [i for i, x in enumerate(taxids) if x in taxid_list]\n",
    "    if (len(check_in)==0):\n",
    "        return (np.nan)\n",
    "    return (taxids[check_in[0]])\n",
    "\n",
    "\n",
    "\n",
    "def clean_taxids(df, taxids, root_taxid, taxid_colname=\"taxid\"):\n",
    "    if isinstance(taxids[0], str):\n",
    "        taxids = dict(zip([ncbi.get_name_translator([x])[x][0] for x in taxids], taxids))\n",
    "    else:\n",
    "        taxids = ncbi.get_taxid_translator(taxids)\n",
    "    if isinstance(root_taxid, str):\n",
    "        root_taxid_number = ncbi.get_name_translator([root_taxid])[root_taxid][0]\n",
    "        root_taxid = {root_taxid_number:root_taxid}\n",
    "    else:\n",
    "        root_taxid = ncbi.get_taxid_translator([root_taxid])\n",
    "    df[taxid_colname] = df[taxid_colname].apply(check_if_in_any_taxid, taxid_list=taxids)\n",
    "    df[taxid_colname][df[taxid_colname].isnull()] = list(root_taxid.keys())[0]\n",
    "    taxids.update(root_taxid)\n",
    "    df[\"sci_name\"] = df[taxid_colname].apply(lambda x: taxids[x])\n",
    "    return (df)\n",
    "    \n",
    "    \n",
    "def get_summary_table (df, colnames, metric):\n",
    "    if (any(df[\"read_prop\"]==0)):\n",
    "        print (df)\n",
    "    df = df.groupby(colnames)[metric].sum().reset_index()\n",
    "    if not isinstance(metric, list):\n",
    "        metric = [metric]\n",
    "    sort_order = colnames+metric\n",
    "    sort_order.remove(\"sample\")\n",
    "    return (df.sort_values(by=sort_order, ascending=False))\n",
    "\n",
    "def correct_read_count(df, decontam_df, taxid, taxid_colname=\"taxid\", search_lower_lineages=False):\n",
    "    if not isinstance(taxid, list):\n",
    "        taxid = [taxid]\n",
    "    taxid = [ncbi.get_name_translator([x])[x][0] if isinstance(x, str) else x for x in taxid]\n",
    "    if search_lower_lineages:\n",
    "        decontam_table = pd.concat([get_rows_taxid(decontam_df, taxid=x, taxid_colname=taxid_colname) for x in taxid])\n",
    "    else:\n",
    "        decontam_table = pd.concat([decontam_df[decontam_df[taxid_colname]==x] for x in taxid if x in decontam_df[taxid_colname].tolist()])\n",
    "    decontam_table = decontam_table.groupby([\"sample\", \"taxid\"])[\"reads\"].sum().reset_index()\n",
    "    outdf = pd.merge(df, decontam_table, how=\"left\")\n",
    "    outdf = outdf[~outdf[\"reads\"].isnull()]\n",
    "    outdf[\"read_prop\"] = outdf[\"reads\"]/outdf[\"read_count\"]*outdf[\"read_prop\"]\n",
    "    outdf = outdf.drop(columns=[\"read_count\", \"reads\"]).rename(columns={\"reads\":\"read_count\"})\n",
    "    return (outdf)\n",
    "\n",
    "def group_at_higher_tax(df, taxonomic_group, family_name, taxid_colname=\"taxid\", family_colname=\"family\"):\n",
    "    groups = {}\n",
    "    for x in taxonomic_group:\n",
    "        if isinstance(x, str):\n",
    "            groups[x] = ncbi.get_name_translator([x])[x][0]\n",
    "        else:\n",
    "            groups[ncbi.get_taxid_translator(x)[x]] = x\n",
    "    family_assignments = {}\n",
    "    for x in df[\"taxid\"].unique():\n",
    "        lin = ncbi.get_lineage(x)\n",
    "        family_assignments[x] = family_name\n",
    "        for key, taxid_x in groups.items():\n",
    "            if taxid_x in lin:\n",
    "                family_assignments[x] = key\n",
    "                break\n",
    "    df[family_colname] = df[\"taxid\"].apply(lambda x: family_assignments[x])\n",
    "    return (df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_qcov_threshold = 0.9\n",
    "metadata_cols = [\"ska_genus\", \"ska_species\", \"collected_by\"]\n",
    "numbers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read counts data\n",
    "idseq_data = pd.read_csv(\"../../data/metadata/idseq_metadata.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load metadata\n",
    "metadata = pd.read_csv(\"../../data/metadata/CMS001_CMS002_MergedAnnotations.csv\", header=0)\n",
    "metadata = pd.merge(metadata, idseq_data[[\"sample\", \"nonhost_reads\", \"total_reads\"]], left_on=\"NewIDseqName\", right_on=\"sample\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load read count data for all contigs\n",
    "contig_stats_all = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/contig_stats_all.tsv\", sep=\"\\t\", header=0)\n",
    "contig_stats_all = pd.merge(contig_stats_all, metadata, how=\"left\", on=\"sample\")\n",
    "contig_stats_all[\"read_prop\"] = contig_stats_all[\"read_count\"]/contig_stats_all[\"nonhost_reads\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load decontam data\n",
    "true_reads = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/decontam_sample_read_counts.tsv\", sep=\"\\t\", header=0)\n",
    "contam_reads = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/sample_contamination.tsv\", sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load LCA data\n",
    "contig_stats_lca_raw = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/contig_stats_lca.tsv\", sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process raw LCA data\n",
    "contig_stats_lca = contig_stats_lca_raw.assign(identity_qcov=(contig_stats_lca_raw[\"identity\"]/100*contig_stats_lca_raw[\"align_length\"]/contig_stats_lca_raw[\"contig_length\"]).apply(lambda x: min(x, 1)))\n",
    "numbers[\"total_num_contigs_with_blast_hits\"] = len(contig_stats_lca)\n",
    "hexapoda_read_counts = contig_stats_lca[contig_stats_lca[\"hexapoda\"]].groupby(\"sample\")[\"read_count\"].sum().reset_index()\n",
    "contig_stats_lca = contig_stats_lca[~(contig_stats_lca[\"hexapoda\"])]\n",
    "numbers[\"total_nonhexapoda_contigs\"] = len(contig_stats_lca)\n",
    "contig_stats_lca = pd.merge(contig_stats_lca, metadata, how=\"left\", on=\"sample\")\n",
    "contig_stats_lca[\"nonhost_reads\"] = pd.merge(contig_stats_lca, hexapoda_read_counts,  how=\"left\", on=\"sample\", suffixes=[\"\", \"_hexapoda\"]).apply(lambda x: x[\"nonhost_reads\"]-x[\"read_count_hexapoda\"], axis=1)\n",
    "contig_stats_lca[\"read_prop\"] = contig_stats_lca[\"read_count\"]/contig_stats_lca[\"nonhost_reads\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated list of known viruses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "(\"ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\", 'occurred at index 1')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-640-f3b02bb1db94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"../../data/darkmatter/virus.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mviral_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mviral_contigs_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviral_json\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_viral_family_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_viral_contigs\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mviral_contigs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Add read proportions columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6002\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6003\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6004\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# compute the result using the series generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-639-e7fdc3caa431>\u001b[0m in \u001b[0;36mget_viral_family_df\u001b[0;34m(row_x)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf_by_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msegments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"contigs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf_by_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_by_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"family\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaxid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"taxid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"provisional_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mdf_by_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_by_sample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msci_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"provisional_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: (\"ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\", 'occurred at index 1')"
     ]
    }
   ],
   "source": [
    "# Information about Baltimore classification of virus family groups\n",
    "viral_family_groups = pd.read_csv(\"../../data/virus_family_groups.csv\", header=0)\n",
    "viral_family_groups = viral_family_groups.loc[:, ~viral_family_groups.columns.str.startswith('Unnamed')]\n",
    "# Convert virus json into data frame\n",
    "with open (\"../../data/darkmatter/virus.json\", \"r\") as f:\n",
    "    viral_json = pd.DataFrame(json.load(f)).T\n",
    "viral_contigs_df = pd.concat(viral_json.apply(get_viral_family_df, axis=1).tolist())\n",
    "numbers[\"num_viral_contigs\"] = len(viral_contigs)\n",
    "# Add read proportions columns\n",
    "viral_contigs = pd.merge(viral_contigs_df, contig_stats_all[[\"sample\", \"read_prop\", \"contig_name\"]], how=\"left\", on=[\"sample\", \"contig_name\"]).groupby([\"sample\", \"sci_name\", \"taxid\"])[\"read_prop\"].sum().reset_index()\n",
    "# Add metadata information\n",
    "viral_contigs = pd.merge(viral_contigs, contig_stats_lca[[\"sample\"]+metadata_cols].groupby([\"sample\"]).first().reset_index(), how=\"left\")\n",
    "viral_contigs = pd.merge(viral_contigs, viral_contigs_df[[\"sample\", \"sci_name\", \"family\"]], how=\"left\")\n",
    "# Add baltimore group information about the viruses\n",
    "viral_contigs = pd.merge(viral_contigs, viral_family_groups, on=\"family\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(viral_json[\"partial\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curate lists of non-viral contigs with high-confidence hits to NCBI records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduct from the nonhost reads per sample the number of reads that were removed due to suspected contamination\n",
    "contig_stats_lca[\"nonhost_reads\"] = pd.merge(contig_stats_lca, contam_reads.groupby(\"sample\")[\"reads\"].sum().reset_index(), how=\"left\").apply(lambda x: x[\"nonhost_reads\"]-x[\"reads\"] if not math.isnan(x[\"reads\"]) else x[\"nonhost_reads\"], axis=1)\n",
    "contig_stats_lca[\"read_prop\"] = contig_stats_lca[\"read_count\"]/contig_stats_lca[\"nonhost_reads\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 197514.0 was translated into 2555385\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 257814.0 was translated into 2608709\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 257814 was translated into 2608709\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n"
     ]
    }
   ],
   "source": [
    "# Only keep hits that are almost identical to a known wolbachia sequence\n",
    "wolbachia_taxid = 952\n",
    "wolbachia_contigs = get_rows_taxid(contig_stats_lca, taxid=wolbachia_taxid, taxid_colname=\"taxid\", identity_qcov_cutoff=identity_qcov_threshold)\n",
    "# Only keep wolbachia groups that were not removed by the decontamination step\n",
    "wolbachia_contigs = pd.merge(wolbachia_contigs, true_reads, how=\"left\")\n",
    "wolbachia_contigs = wolbachia_contigs[~wolbachia_contigs[\"reads\"].isnull()]\n",
    "numbers[\"total_wolbachia_contigs\"] = len(wolbachia_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "wolbachia_contigs = get_summary_table(wolbachia_contigs, colnames=[\"ska_genus\", \"ska_species\", \"collected_by\", \"sample\", \"taxid\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# Correct read counts after decontamination\n",
    "wolbachia_contigs = correct_read_count(wolbachia_contigs, true_reads, wolbachia_taxid, \"taxid\", search_lower_lineages=True)\n",
    "# Create sci_name column to denote that that this table contains Wolbachia samples\n",
    "wolbachia_contigs = wolbachia_contigs.assign(sci_name=ncbi.get_taxid_translator([wolbachia_taxid])[wolbachia_taxid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep hits that are almost identical to a known metazoan sequence\n",
    "metazoan_contigs = contig_stats_lca[(contig_stats_lca[\"taxon_group\"]==\"Metazoa\") & (contig_stats_lca[\"identity_qcov\"]>=identity_qcov_threshold)]\n",
    "# Only keep metazoan groups that were not removed by the decontamination step\n",
    "metazoan_contigs = pd.merge(metazoan_contigs, true_reads, how=\"left\")\n",
    "metazoan_contigs = metazoan_contigs[~metazoan_contigs[\"reads\"].isnull()]\n",
    "# Convert taxids to those of interest\n",
    "#metazoan_contigs = clean_taxids(metazoan_contigs, taxids=[\"Leporidae\", \"Muroidea\", \"Homo sapiens\", \"Carnivora\", \"Odocoileinae\", \"Bovidae\", \"Neognathae\"], root_taxid=\"Metazoa\")\n",
    "metazoan_contigs = metazoan_contigs.assign(sci_name=metazoan_contigs[\"taxid\"].apply(lambda x: ncbi.get_taxid_translator([x])[x]))\n",
    "numbers[\"total_metazoan_contigs\"] = len(metazoan_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "metazoan_contigs = get_summary_table(metazoan_contigs, colnames=metadata_cols+[\"sample\", \"taxid\", \"sci_name\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# Correct read counts after decontamination\n",
    "metazoan_taxids = metazoan_contigs[\"taxid\"].unique().tolist()\n",
    "metazoan_contigs = correct_read_count(metazoan_contigs, true_reads, taxid=metazoan_taxids, taxid_colname=\"taxid\", search_lower_lineages=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 197514.0 was translated into 2555385\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 257814.0 was translated into 2608709\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n"
     ]
    }
   ],
   "source": [
    "# Only keep hits that are almost identical to a known chordate sequence\n",
    "# chordate_contigs = contig_stats_lca[(contig_stats_lca[\"taxon_group\"]==\"Metazoa\") & (contig_stats_lca[\"identity_qcov\"]>=identity_qcov_threshold)]\n",
    "chordate_contigs = get_rows_taxid(contig_stats_lca, taxid=\"Chordata\", taxid_colname=\"taxid\", identity_qcov_cutoff=identity_qcov_threshold)\n",
    "# Only keep metazoan groups that were not removed by the decontamination step\n",
    "chordate_contigs = pd.merge(chordate_contigs, true_reads, how=\"left\")\n",
    "chordate_contigs = chordate_contigs[~chordate_contigs[\"reads\"].isnull()]\n",
    "# Convert taxids to those of interest\n",
    "chordate_contigs = chordate_contigs.assign(sci_name=chordate_contigs[\"taxid\"].apply(lambda x: ncbi.get_taxid_translator([x])[x]))\n",
    "numbers[\"total_chordate_contigs\"] = len(chordate_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "chordate_contigs = get_summary_table(chordate_contigs, colnames=metadata_cols+[\"sample\", \"taxid\", \"sci_name\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# Correct read counts after decontamination\n",
    "chordate_taxids = chordate_contigs[\"taxid\"].unique().tolist()\n",
    "chordate_contigs = correct_read_count(chordate_contigs, true_reads, taxid=chordate_taxids, taxid_colname=\"taxid\", search_lower_lineages=False)\n",
    "# In the 'family' column, add information about the higher taxonomic grouping for taxids\n",
    "chordate_contigs = group_at_higher_tax(chordate_contigs, taxonomic_group=[\"Pecora\", \"Aves\", \"Carnivora\", \"Rodentia\", \"Leporidae\"], family_name=\"Other chordates\", taxid_colname=\"taxid\", family_colname=\"family\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep hits that are almost identical to a known eukaryote sequence\n",
    "eukaryotic_contigs_df = contig_stats_lca[(contig_stats_lca[\"taxon_group\"]==\"Eukaryota\") & (contig_stats_lca[\"identity_qcov\"]>=identity_qcov_threshold)]\n",
    "# Only keep eukaryotic groups that were not removed by the decontamination step\n",
    "eukaryotic_contigs_df = pd.merge(eukaryotic_contigs_df, true_reads, how=\"left\")\n",
    "eukaryotic_contigs_df = eukaryotic_contigs_df[~eukaryotic_contigs_df[\"reads\"].isnull()]\n",
    "# Convert taxids to those of interest\n",
    "eukaryotic_contigs = eukaryotic_contigs_df.assign(sci_name=eukaryotic_contigs_df[\"taxid\"].apply(lambda x: ncbi.get_taxid_translator([x])[x]))\n",
    "numbers[\"total_eukaryotic_contigs\"] = len(eukaryotic_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "eukaryotic_contigs = get_summary_table(eukaryotic_contigs, colnames=metadata_cols+[\"sample\", \"taxid\", \"sci_name\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# Correct read counts after decontamination\n",
    "eukaryotic_taxids = eukaryotic_contigs[\"taxid\"].unique().tolist()\n",
    "eukaryotic_contigs = correct_read_count(eukaryotic_contigs, true_reads, taxid=eukaryotic_taxids, taxid_colname=\"taxid\", search_lower_lineages=False) \n",
    "# In the 'family' column, add information about the higher taxonomic grouping for taxids\n",
    "eukaryotic_contigs = group_at_higher_tax(eukaryotic_contigs, taxonomic_group=[\"Trypanosomatidae\", \"Apicomplexa\", \"Amblyosporidae\"], family_name=\"Other eukaryotes\", taxid_colname=\"taxid\", family_colname=\"family\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "all_contigs_df = pd.concat([wolbachia_contigs.assign(group=\"Wolbachia\"), \n",
    "                            viral_contigs.assign(group=\"Virus\"),\n",
    "                            metazoan_contigs.assign(group=\"Metazoa\"), \n",
    "                            eukaryotic_contigs.assign(group=\"Other Eukaryotes\")])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contigs_df.to_csv(\"../../figures/fig3/all_contigs_df.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexapoda_decontam_reads = contig_stats_lca[[\"sample\", \"nonhost_reads\"]][(~contig_stats_lca[\"sample\"].duplicated()) & (~contig_stats_lca[\"nonhost_reads\"].isnull())]\n",
    "hexapoda_decontam_reads = hexapoda_decontam_reads.astype({\"nonhost_reads\":int})\n",
    "hexapoda_decontam_reads.to_csv(\"../../data/metadata/nonhost_reads_decontam_nohexapoda.tsv\", index=False, sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
