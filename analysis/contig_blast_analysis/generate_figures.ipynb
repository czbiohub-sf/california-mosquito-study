{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ete3 import NCBITaxa\n",
    "import boto3\n",
    "import tempfile\n",
    "import subprocess\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "ncbi = NCBITaxa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_viral_family_df (row_x):\n",
    "    segments = row_x[\"segments\"]\n",
    "    df_by_sample = pd.DataFrame([x.split(\"|\") for x in segments[list(segments.keys())[0]][\"contigs\"]])\n",
    "    df_by_sample = df_by_sample.assign(family=row_x[\"family\"], taxid=row_x[\"taxid\"])\n",
    "    if not pd.isnull(row_x[\"provisional_name\"]):\n",
    "        df_by_sample = df_by_sample.assign(sci_name=row_x[\"provisional_name\"])\n",
    "    else:\n",
    "        df_by_sample = df_by_sample.assign(sci_name=row_x[\"name\"])\n",
    "    df_by_sample = df_by_sample.assign(poly_group=row_x[\"poly_group\"]).astype({\"poly_group\":int})\n",
    "    df_by_sample = df_by_sample.rename(columns={0:\"sample\", 1:\"contig_name\"})\n",
    "    return (df_by_sample)\n",
    "\n",
    "def get_rows_taxid (df, taxid, taxid_colname=\"taxid\", identity_qcov_cutoff=None):\n",
    "    if (not isinstance(df, pd.DataFrame)):\n",
    "        if isinstance(taxid, str):\n",
    "            taxid = ncbi.get_name_translator([taxid])[taxid][0]\n",
    "        return (taxid in ncbi.get_lineage(df))\n",
    "    outdf = df[df[taxid_colname].apply(get_rows_taxid, taxid=taxid)]\n",
    "    if identity_qcov_cutoff is not None:\n",
    "        outdf = outdf[outdf[\"identity_qcov\"]>=identity_qcov_cutoff]\n",
    "    return (outdf)\n",
    "\n",
    "\n",
    "def check_if_in_any_taxid(taxid, taxid_list):\n",
    "    if taxid in taxid_list:\n",
    "        return (taxid)\n",
    "    taxids = ncbi.get_lineage(taxid)\n",
    "    check_in = [i for i, x in enumerate(taxids) if x in taxid_list]\n",
    "    if (len(check_in)==0):\n",
    "        return (np.nan)\n",
    "    return (taxids[check_in[0]])\n",
    "\n",
    "\n",
    "\n",
    "def clean_taxids(df, taxids, root_taxid, taxid_colname=\"taxid\"):\n",
    "    if isinstance(taxids[0], str):\n",
    "        taxids = dict(zip([ncbi.get_name_translator([x])[x][0] for x in taxids], taxids))\n",
    "    else:\n",
    "        taxids = ncbi.get_taxid_translator(taxids)\n",
    "    if isinstance(root_taxid, str):\n",
    "        root_taxid_number = ncbi.get_name_translator([root_taxid])[root_taxid][0]\n",
    "        root_taxid = {root_taxid_number:root_taxid}\n",
    "    else:\n",
    "        root_taxid = ncbi.get_taxid_translator([root_taxid])\n",
    "    df[taxid_colname] = df[taxid_colname].apply(check_if_in_any_taxid, taxid_list=taxids)\n",
    "    df[taxid_colname][df[taxid_colname].isnull()] = list(root_taxid.keys())[0]\n",
    "    taxids.update(root_taxid)\n",
    "    df[\"sci_name\"] = df[taxid_colname].apply(lambda x: taxids[x])\n",
    "    return (df)\n",
    "    \n",
    "    \n",
    "def get_summary_table (df, colnames, metric):\n",
    "    if (any(df[\"read_prop\"]==0)):\n",
    "        print (df)\n",
    "    df = df.groupby(colnames)[metric].sum().reset_index()\n",
    "    if not isinstance(metric, list):\n",
    "        metric = [metric]\n",
    "    sort_order = colnames+metric\n",
    "    sort_order.remove(\"sample\")\n",
    "    return (df.sort_values(by=sort_order, ascending=False))\n",
    "\n",
    "\n",
    "def group_at_higher_tax(df, taxonomic_group, family_name, taxid_colname=\"taxid\", family_colname=\"family\"):\n",
    "    groups = {}\n",
    "    for x in taxonomic_group:\n",
    "        if isinstance(x, str):\n",
    "            groups[x] = ncbi.get_name_translator([x])[x][0]\n",
    "        else:\n",
    "            groups[ncbi.get_taxid_translator(x)[x]] = x\n",
    "    family_assignments = {}\n",
    "    for x in df[\"taxid\"].unique():\n",
    "        lin = ncbi.get_lineage(x)\n",
    "        family_assignments[x] = family_name\n",
    "        for key, taxid_x in groups.items():\n",
    "            if taxid_x in lin:\n",
    "                family_assignments[x] = key\n",
    "                break\n",
    "    df[family_colname] = df[\"taxid\"].apply(lambda x: family_assignments[x])\n",
    "    return (df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_qcov_threshold = 0.9\n",
    "metadata_cols = [\"ska_genus\", \"ska_species\", \"collected_by\"]\n",
    "numbers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read counts data\n",
    "idseq_data = pd.read_csv(\"../../data/metadata/idseq_metadata.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load metadata\n",
    "metadata = pd.read_csv(\"../../data/metadata/CMS001_CMS002_MergedAnnotations.csv\", header=0)\n",
    "metadata = pd.merge(metadata, idseq_data[[\"sample\", \"nonhost_reads\", \"total_reads\"]], left_on=\"NewIDseqName\", right_on=\"sample\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load read count data for all contigs\n",
    "contig_stats_all = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/contig_stats_all.tsv\", sep=\"\\t\", header=0)\n",
    "contig_stats_all = pd.merge(contig_stats_all, metadata, how=\"left\", on=\"sample\")\n",
    "contig_stats_all[\"read_prop\"] = contig_stats_all[\"read_count\"]/contig_stats_all[\"nonhost_reads\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load decontam data\n",
    "true_reads = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/lca_decontam.tsv\", sep=\"\\t\", header=0)\n",
    "contam_reads = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/lca_contamination.tsv\", sep=\"\\t\", header=0)\n",
    "#viral_contam_reads = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/viral_contamination.tsv\", sep=\"\\t\", header=0)\n",
    "true_viral_reads = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/viral_decontam.tsv\", sep=\"\\t\", header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load LCA data\n",
    "contig_stats_lca_raw = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/contig_stats_lca.tsv\", sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process raw LCA data\n",
    "contig_stats_lca = contig_stats_lca_raw.assign(identity_qcov=(contig_stats_lca_raw[\"identity\"]/100*contig_stats_lca_raw[\"align_length\"]/contig_stats_lca_raw[\"contig_length\"]).apply(lambda x: min(x, 1)))\n",
    "numbers[\"total_num_contigs_with_blast_hits\"] = len(contig_stats_lca)\n",
    "hexapoda_read_counts = contig_stats_lca[contig_stats_lca[\"hexapoda\"]].groupby(\"sample\")[\"read_count\"].sum().reset_index()\n",
    "contig_stats_lca = contig_stats_lca[~(contig_stats_lca[\"hexapoda\"])]\n",
    "numbers[\"total_nonhexapoda_contigs\"] = len(contig_stats_lca)\n",
    "contig_stats_lca = pd.merge(contig_stats_lca, metadata, how=\"left\", on=\"sample\")\n",
    "contig_stats_lca[\"nonhost_reads\"] = pd.merge(contig_stats_lca, hexapoda_read_counts,  how=\"left\", on=\"sample\", suffixes=[\"\", \"_hexapoda\"]).apply(lambda x: x[\"nonhost_reads\"]-x[\"read_count_hexapoda\"], axis=1)\n",
    "contig_stats_lca[\"read_prop\"] = contig_stats_lca[\"read_count\"]/contig_stats_lca[\"nonhost_reads\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated list of known viruses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Information about Baltimore classification of virus family groups\n",
    "viral_family_groups = pd.read_csv(\"../../data/virus_family_groups.csv\", header=0)\n",
    "viral_family_groups = viral_family_groups.loc[:, ~viral_family_groups.columns.str.startswith('Unnamed')]\n",
    "# Convert virus json into data frame\n",
    "with open (\"../../data/darkmatter/virus.json\", \"r\") as f:\n",
    "    viral_json = pd.DataFrame(json.load(f)).T\n",
    "    viral_json['poly_group'] = viral_json.index\n",
    "viral_contigs_df = pd.concat(viral_json.apply(get_viral_family_df, axis=1).tolist())\n",
    "numbers[\"num_viral_contigs\"] = len(viral_contigs_df)\n",
    "# Add read proportions columns\n",
    "viral_contigs = pd.merge(viral_contigs_df, contig_stats_all[[\"sample\", \"read_prop\", \"contig_name\"]], how=\"left\", on=[\"sample\", \"contig_name\"]).groupby([\"sample\", \"sci_name\", \"taxid\", \"poly_group\"])[\"read_prop\"].sum().reset_index()\n",
    "# Exclude viruses labelled as contamination\n",
    "viral_contigs = pd.merge(viral_contigs, true_viral_reads, how=\"left\", on=[\"sample\", \"poly_group\"])\n",
    "viral_contigs = viral_contigs[~viral_contigs[\"reads\"].isnull()]\n",
    "# Add metadata information\n",
    "viral_contigs = pd.merge(viral_contigs, contig_stats_lca[[\"sample\"]+metadata_cols].groupby([\"sample\"]).first().reset_index(), how=\"left\")\n",
    "viral_contigs = pd.merge(viral_contigs, viral_contigs_df[[\"sample\", \"sci_name\", \"family\"]], how=\"left\")\n",
    "# Add baltimore group information about the viruses\n",
    "viral_contigs = pd.merge(viral_contigs, viral_family_groups, on=\"family\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curate lists of non-viral contigs with high-confidence hits to NCBI records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduct from the nonhost reads per sample the number of reads that were removed due to suspected contamination\n",
    "contig_stats_lca[\"nonhost_reads\"] = pd.merge(contig_stats_lca, contam_reads.groupby(\"sample\")[\"reads\"].sum().reset_index(), how=\"left\").apply(lambda x: x[\"nonhost_reads\"]-x[\"reads\"] if not math.isnan(x[\"reads\"]) else x[\"nonhost_reads\"], axis=1)\n",
    "contig_stats_lca[\"read_prop\"] = contig_stats_lca[\"read_count\"]/contig_stats_lca[\"nonhost_reads\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 197514.0 was translated into 2555385\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 257814.0 was translated into 2608709\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n"
     ]
    }
   ],
   "source": [
    "# Only keep hits that are almost identical to a known wolbachia sequence\n",
    "wolbachia_taxid = 952\n",
    "wolbachia_contigs = get_rows_taxid(contig_stats_lca, taxid=wolbachia_taxid, taxid_colname=\"taxid\", identity_qcov_cutoff=identity_qcov_threshold)\n",
    "# Only keep wolbachia groups that were not removed by the decontamination step\n",
    "wolbachia_contigs = pd.merge(wolbachia_contigs, true_reads, how=\"left\")\n",
    "wolbachia_contigs = wolbachia_contigs[~wolbachia_contigs[\"reads\"].isnull()]\n",
    "numbers[\"total_wolbachia_contigs\"] = len(wolbachia_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "wolbachia_contigs = get_summary_table(wolbachia_contigs, colnames=[\"ska_genus\", \"ska_species\", \"collected_by\", \"sample\", \"taxid\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# rename column to 'reads' for consistency\n",
    "wolbachia_contigs = wolbachia_contigs.rename(columns={\"read_count\":\"reads\"})\n",
    "# Create sci_name column to denote that that this table contains Wolbachia samples\n",
    "wolbachia_contigs = wolbachia_contigs.assign(sci_name=ncbi.get_taxid_translator([wolbachia_taxid])[wolbachia_taxid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep hits that are almost identical to a known metazoan sequence\n",
    "metazoan_contigs = contig_stats_lca[(contig_stats_lca[\"taxon_group\"]==\"Metazoa\") & (contig_stats_lca[\"identity_qcov\"]>=identity_qcov_threshold)]\n",
    "# Only keep metazoan groups that were not removed by the decontamination step\n",
    "metazoan_contigs = pd.merge(metazoan_contigs, true_reads, how=\"left\")\n",
    "metazoan_contigs = metazoan_contigs[~metazoan_contigs[\"reads\"].isnull()]\n",
    "# Convert taxids to those of interest\n",
    "#metazoan_contigs = clean_taxids(metazoan_contigs, taxids=[\"Leporidae\", \"Muroidea\", \"Homo sapiens\", \"Carnivora\", \"Odocoileinae\", \"Bovidae\", \"Neognathae\"], root_taxid=\"Metazoa\")\n",
    "metazoan_contigs = metazoan_contigs.assign(sci_name=metazoan_contigs[\"taxid\"].apply(lambda x: ncbi.get_taxid_translator([x])[x]))\n",
    "numbers[\"total_metazoan_contigs\"] = len(metazoan_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "metazoan_contigs = get_summary_table(metazoan_contigs, colnames=metadata_cols+[\"sample\", \"taxid\", \"sci_name\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# rename column to 'reads' for consistency\n",
    "metazoan_contigs = metazoan_contigs.rename(columns={\"read_count\":\"reads\"})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 197514.0 was translated into 2555385\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 257814.0 was translated into 2608709\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n"
     ]
    }
   ],
   "source": [
    "# Only keep hits that are almost identical to a known chordate sequence\n",
    "# chordate_contigs = contig_stats_lca[(contig_stats_lca[\"taxon_group\"]==\"Metazoa\") & (contig_stats_lca[\"identity_qcov\"]>=identity_qcov_threshold)]\n",
    "chordate_contigs = get_rows_taxid(contig_stats_lca, taxid=\"Chordata\", taxid_colname=\"taxid\", identity_qcov_cutoff=identity_qcov_threshold)\n",
    "# Only keep metazoan groups that were not removed by the decontamination step\n",
    "chordate_contigs = pd.merge(chordate_contigs, true_reads, how=\"left\")\n",
    "chordate_contigs = chordate_contigs[~chordate_contigs[\"reads\"].isnull()]\n",
    "# Convert taxids to those of interest\n",
    "chordate_contigs = chordate_contigs.assign(sci_name=chordate_contigs[\"taxid\"].apply(lambda x: ncbi.get_taxid_translator([x])[x]))\n",
    "numbers[\"total_chordate_contigs\"] = len(chordate_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "chordate_contigs = get_summary_table(chordate_contigs, colnames=metadata_cols+[\"sample\", \"taxid\", \"sci_name\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# rename column to 'reads' for consistency\n",
    "chordate_contigs = chordate_contigs.rename(columns={\"read_count\":\"reads\"})\n",
    "# In the 'family' column, add information about the higher taxonomic grouping for taxids\n",
    "chordate_contigs = group_at_higher_tax(chordate_contigs, taxonomic_group=[\"Pecora\", \"Aves\", \"Carnivora\", \"Rodentia\", \"Leporidae\"], family_name=\"Other chordates\", taxid_colname=\"taxid\", family_colname=\"family\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep hits that are almost identical to a known eukaryote sequence\n",
    "eukaryotic_contigs_df = contig_stats_lca[(contig_stats_lca[\"taxon_group\"]==\"Eukaryota\") & (contig_stats_lca[\"identity_qcov\"]>=identity_qcov_threshold)]\n",
    "# Only keep eukaryotic groups that were not removed by the decontamination step\n",
    "eukaryotic_contigs_df = pd.merge(eukaryotic_contigs_df, true_reads, how=\"left\")\n",
    "eukaryotic_contigs_df = eukaryotic_contigs_df[~eukaryotic_contigs_df[\"reads\"].isnull()]\n",
    "# Convert taxids to those of interest\n",
    "eukaryotic_contigs = eukaryotic_contigs_df.assign(sci_name=eukaryotic_contigs_df[\"taxid\"].apply(lambda x: ncbi.get_taxid_translator([x])[x]))\n",
    "numbers[\"total_eukaryotic_contigs\"] = len(eukaryotic_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "eukaryotic_contigs = get_summary_table(eukaryotic_contigs, colnames=metadata_cols+[\"sample\", \"taxid\", \"sci_name\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# rename column to 'reads' for consistency\n",
    "eukaryotic_contigs = eukaryotic_contigs.rename(columns={\"read_count\":\"reads\"})\n",
    "# In the 'family' column, add information about the higher taxonomic grouping for taxids\n",
    "eukaryotic_contigs = group_at_higher_tax(eukaryotic_contigs, taxonomic_group=[\"Trypanosomatidae\", \"Apicomplexa\", \"Amblyosporidae\"], family_name=\"Other eukaryotes\", taxid_colname=\"taxid\", family_colname=\"family\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucy.li/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "all_contigs_df = pd.concat([wolbachia_contigs.assign(group=\"Wolbachia\"), \n",
    "                            viral_contigs.assign(group=\"Virus\"),\n",
    "                            metazoan_contigs.assign(group=\"Metazoa\"),\n",
    "                            chordate_contigs.assign(group=\"Chordates\"),\n",
    "                            eukaryotic_contigs.assign(group=\"Other Eukaryotes\")])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contigs_df.to_csv(\"../../figures/fig3/all_contigs_df.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexapoda_decontam_reads = contig_stats_lca[[\"sample\", \"nonhost_reads\"]][(~contig_stats_lca[\"sample\"].duplicated()) & (~contig_stats_lca[\"nonhost_reads\"].isnull())]\n",
    "hexapoda_decontam_reads = hexapoda_decontam_reads.astype({\"nonhost_reads\":int})\n",
    "hexapoda_decontam_reads.to_csv(\"../../data/metadata/nonhost_reads_decontam_nohexapoda.tsv\", index=False, sep=\"\\t\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
