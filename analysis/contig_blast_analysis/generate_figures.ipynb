{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NCBI database not present yet (first time used?)\n",
      "Downloading taxdump.tar.gz from NCBI FTP site (via HTTP)...\n",
      "Done. Parsing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading node names...\n",
      "2193687 names loaded.\n",
      "213186 synonyms loaded.\n",
      "Loading nodes...\n",
      "2193687 nodes loaded.\n",
      "Linking nodes...\n",
      "Tree is loaded.\n",
      "Updating database: /Users/Lucy/.etetoolkit/taxa.sqlite ...\n",
      " 2193000 generating entries... 18000 generating entries... generating entries...  generating entries... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting synonyms:      20000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading to /Users/Lucy/.etetoolkit/taxa.sqlite\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting taxid merges:  35000   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting taxids:       25000  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting taxids:       2190000 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ete3 import NCBITaxa\n",
    "import boto3\n",
    "import tempfile\n",
    "import subprocess\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "ncbi = NCBITaxa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_viral_family_df (row_x):\n",
    "    segments = row_x[\"segments\"]\n",
    "    df_by_sample = pd.DataFrame([x.split(\"|\") for x in segments[list(segments.keys())[0]][\"contigs\"]])\n",
    "    df_by_sample = df_by_sample.assign(family=row_x[\"family\"], taxid=row_x[\"taxid\"])\n",
    "    if not pd.isnull(row_x[\"provisional_name\"]):\n",
    "        df_by_sample = df_by_sample.assign(sci_name=row_x[\"provisional_name\"])\n",
    "    else:\n",
    "        df_by_sample = df_by_sample.assign(sci_name=row_x[\"name\"])\n",
    "    df_by_sample = df_by_sample.assign(poly_group=row_x[\"poly_group\"]).astype({\"poly_group\":int})\n",
    "    df_by_sample = df_by_sample.rename(columns={0:\"sample\", 1:\"contig_name\"})\n",
    "    return (df_by_sample)\n",
    "\n",
    "def get_rows_taxid (df, taxid, taxid_colname=\"taxid\", identity_qcov_cutoff=None):\n",
    "    if (not isinstance(df, pd.DataFrame)):\n",
    "        if isinstance(taxid, str):\n",
    "            taxid = ncbi.get_name_translator([taxid])[taxid][0]\n",
    "        return (taxid in ncbi.get_lineage(df))\n",
    "    outdf = df[df[taxid_colname].apply(get_rows_taxid, taxid=taxid)]\n",
    "    if identity_qcov_cutoff is not None:\n",
    "        outdf = outdf[outdf[\"identity_qcov\"]>=identity_qcov_cutoff]\n",
    "    return (outdf)\n",
    "\n",
    "\n",
    "# def filter_by_criterion (df, colname, minthreshold, bysample=True):\n",
    "#     if bysample:\n",
    "#         sums = df.groupby([\"sample\", \"taxid\"])[colname].sum().reset_index()\n",
    "#         sums[\"tokeep\"] = sums[colname] >= minthreshold\n",
    "#         df = pd.merge(df, sums.drop(columns=colname), how=\"left\")\n",
    "#         df = df[df[\"tokeep\"]!=False].drop(columns=\"tokeep\")\n",
    "#     else:\n",
    "#         df = df[df[colname] >= minthreshold]\n",
    "#     return (df)\n",
    "\n",
    "\n",
    "def check_if_in_any_taxid(taxid, taxid_list):\n",
    "    if taxid in taxid_list:\n",
    "        return (taxid)\n",
    "    taxids = ncbi.get_lineage(taxid)\n",
    "    check_in = [i for i, x in enumerate(taxids) if x in taxid_list]\n",
    "    if (len(check_in)==0):\n",
    "        return (np.nan)\n",
    "    return (taxids[check_in[0]])\n",
    "\n",
    "\n",
    "\n",
    "def clean_taxids(df, taxids, root_taxid, taxid_colname=\"taxid\"):\n",
    "    if isinstance(taxids[0], str):\n",
    "        taxids = dict(zip([ncbi.get_name_translator([x])[x][0] for x in taxids], taxids))\n",
    "    else:\n",
    "        taxids = ncbi.get_taxid_translator(taxids)\n",
    "    if isinstance(root_taxid, str):\n",
    "        root_taxid_number = ncbi.get_name_translator([root_taxid])[root_taxid][0]\n",
    "        root_taxid = {root_taxid_number:root_taxid}\n",
    "    else:\n",
    "        root_taxid = ncbi.get_taxid_translator([root_taxid])\n",
    "    df[taxid_colname] = df[taxid_colname].apply(check_if_in_any_taxid, taxid_list=taxids)\n",
    "    df[taxid_colname][df[taxid_colname].isnull()] = list(root_taxid.keys())[0]\n",
    "    taxids.update(root_taxid)\n",
    "    df[\"sci_name\"] = df[taxid_colname].apply(lambda x: taxids[x])\n",
    "    return (df)\n",
    "    \n",
    "    \n",
    "def get_summary_table (df, colnames, metric):\n",
    "    if (any(df[\"read_prop\"]==0)):\n",
    "        print (df)\n",
    "    df = df.groupby(colnames)[metric].sum().reset_index()\n",
    "    if not isinstance(metric, list):\n",
    "        metric = [metric]\n",
    "    sort_order = colnames+metric\n",
    "    sort_order.remove(\"sample\")\n",
    "    return (df.sort_values(by=sort_order, ascending=False))\n",
    "\n",
    "def correct_read_count(df, decontam_df, taxid, taxid_colname=\"taxid\", search_lower_lineages=False):\n",
    "    if not isinstance(taxid, list):\n",
    "        taxid = [taxid]\n",
    "    taxid = [ncbi.get_name_translator([x])[x][0] if isinstance(x, str) else x for x in taxid]\n",
    "    if search_lower_lineages:\n",
    "        decontam_table = pd.concat([get_rows_taxid(decontam_df, taxid=x, taxid_colname=taxid_colname) for x in taxid])\n",
    "    else:\n",
    "        decontam_table = pd.concat([decontam_df[decontam_df[taxid_colname]==x] for x in taxid if x in decontam_df[taxid_colname].tolist()])\n",
    "    decontam_table = decontam_table.groupby([\"sample\", \"taxid\"])[\"reads\"].sum().reset_index()\n",
    "    outdf = pd.merge(df, decontam_table, how=\"left\")\n",
    "    outdf = outdf[~outdf[\"reads\"].isnull()]\n",
    "    outdf[\"read_prop\"] = outdf[\"reads\"]/outdf[\"read_count\"]*outdf[\"read_prop\"]\n",
    "    outdf = outdf.drop(columns=[\"read_count\", \"reads\"]).rename(columns={\"reads\":\"read_count\"})\n",
    "    return (outdf)\n",
    "\n",
    "def group_at_higher_tax(df, taxonomic_group, family_name, taxid_colname=\"taxid\", family_colname=\"family\"):\n",
    "    groups = {}\n",
    "    for x in taxonomic_group:\n",
    "        if isinstance(x, str):\n",
    "            groups[x] = ncbi.get_name_translator([x])[x][0]\n",
    "        else:\n",
    "            groups[ncbi.get_taxid_translator(x)[x]] = x\n",
    "    family_assignments = {}\n",
    "    for x in df[\"taxid\"].unique():\n",
    "        lin = ncbi.get_lineage(x)\n",
    "        family_assignments[x] = family_name\n",
    "        for key, taxid_x in groups.items():\n",
    "            if taxid_x in lin:\n",
    "                family_assignments[x] = key\n",
    "                break\n",
    "    df[family_colname] = df[\"taxid\"].apply(lambda x: family_assignments[x])\n",
    "    return (df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_qcov_threshold = 0.9\n",
    "metadata_cols = [\"ska_genus\", \"ska_species\", \"collected_by\"]\n",
    "numbers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read counts data\n",
    "idseq_data = pd.read_csv(\"../../data/metadata/idseq_metadata.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load metadata\n",
    "metadata = pd.read_csv(\"../../data/metadata/CMS001_CMS002_MergedAnnotations.csv\", header=0)\n",
    "metadata = pd.merge(metadata, idseq_data[[\"sample\", \"nonhost_reads\", \"total_reads\"]], left_on=\"NewIDseqName\", right_on=\"sample\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load read count data for all contigs\n",
    "contig_stats_all = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/contig_stats_all.tsv\", sep=\"\\t\", header=0)\n",
    "contig_stats_all = pd.merge(contig_stats_all, metadata, how=\"left\", on=\"sample\")\n",
    "contig_stats_all[\"read_prop\"] = contig_stats_all[\"read_count\"]/contig_stats_all[\"nonhost_reads\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load decontam data\n",
    "true_reads = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/decontam_sample_read_counts.tsv\", sep=\"\\t\", header=0)\n",
    "contam_reads = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/sample_contamination.tsv\", sep=\"\\t\", header=0)\n",
    "viral_contam_reads = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/viral_contamination.tsv\", sep=\"\\t\", header=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load LCA data\n",
    "contig_stats_lca_raw = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/contig_stats_lca.tsv\", sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process raw LCA data\n",
    "contig_stats_lca = contig_stats_lca_raw.assign(identity_qcov=(contig_stats_lca_raw[\"identity\"]/100*contig_stats_lca_raw[\"align_length\"]/contig_stats_lca_raw[\"contig_length\"]).apply(lambda x: min(x, 1)))\n",
    "numbers[\"total_num_contigs_with_blast_hits\"] = len(contig_stats_lca)\n",
    "hexapoda_read_counts = contig_stats_lca[contig_stats_lca[\"hexapoda\"]].groupby(\"sample\")[\"read_count\"].sum().reset_index()\n",
    "contig_stats_lca = contig_stats_lca[~(contig_stats_lca[\"hexapoda\"])]\n",
    "numbers[\"total_nonhexapoda_contigs\"] = len(contig_stats_lca)\n",
    "contig_stats_lca = pd.merge(contig_stats_lca, metadata, how=\"left\", on=\"sample\")\n",
    "contig_stats_lca[\"nonhost_reads\"] = pd.merge(contig_stats_lca, hexapoda_read_counts,  how=\"left\", on=\"sample\", suffixes=[\"\", \"_hexapoda\"]).apply(lambda x: x[\"nonhost_reads\"]-x[\"read_count_hexapoda\"], axis=1)\n",
    "contig_stats_lca[\"read_prop\"] = contig_stats_lca[\"read_count\"]/contig_stats_lca[\"nonhost_reads\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated list of known viruses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample</th>\n",
       "      <th>poly_group</th>\n",
       "      <th>reads</th>\n",
       "      <th>background_reads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CMS002_038a_Rb_S172_L004</td>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>36.030401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CMS002_040a_Rb_S174_L004</td>\n",
       "      <td>44</td>\n",
       "      <td>591</td>\n",
       "      <td>58.269700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CMS002_046a_Rb_S191_L004</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>11.478392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CMS002_046b_Rb_S192_L004</td>\n",
       "      <td>44</td>\n",
       "      <td>96</td>\n",
       "      <td>12.280238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CMS002_042a_Rb_S177_L004</td>\n",
       "      <td>458</td>\n",
       "      <td>1340</td>\n",
       "      <td>19.906068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CMS002_044a_Rb_S178_L004</td>\n",
       "      <td>458</td>\n",
       "      <td>179</td>\n",
       "      <td>46.446082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CMS002_044b_Rb_S179_L004</td>\n",
       "      <td>458</td>\n",
       "      <td>18</td>\n",
       "      <td>110.933900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CMS002_044d_Rb_S181_L004</td>\n",
       "      <td>458</td>\n",
       "      <td>97</td>\n",
       "      <td>105.598081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CMS002_044e_Rb_S182_L004</td>\n",
       "      <td>458</td>\n",
       "      <td>147</td>\n",
       "      <td>246.721010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CMS002_045g_Rb_S190_L004</td>\n",
       "      <td>458</td>\n",
       "      <td>103</td>\n",
       "      <td>54.393211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CMS002_028a_Rb_S154_L004</td>\n",
       "      <td>493</td>\n",
       "      <td>31</td>\n",
       "      <td>2.595480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CMS002_031a_Rb_S165_L004</td>\n",
       "      <td>493</td>\n",
       "      <td>114</td>\n",
       "      <td>13.261279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CMS002_038a_Rb_S172_L004</td>\n",
       "      <td>493</td>\n",
       "      <td>22</td>\n",
       "      <td>8.685504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      sample  poly_group  reads  background_reads\n",
       "0   CMS002_038a_Rb_S172_L004          44     10         36.030401\n",
       "1   CMS002_040a_Rb_S174_L004          44    591         58.269700\n",
       "2   CMS002_046a_Rb_S191_L004          44      9         11.478392\n",
       "3   CMS002_046b_Rb_S192_L004          44     96         12.280238\n",
       "4   CMS002_042a_Rb_S177_L004         458   1340         19.906068\n",
       "5   CMS002_044a_Rb_S178_L004         458    179         46.446082\n",
       "6   CMS002_044b_Rb_S179_L004         458     18        110.933900\n",
       "7   CMS002_044d_Rb_S181_L004         458     97        105.598081\n",
       "8   CMS002_044e_Rb_S182_L004         458    147        246.721010\n",
       "9   CMS002_045g_Rb_S190_L004         458    103         54.393211\n",
       "10  CMS002_028a_Rb_S154_L004         493     31          2.595480\n",
       "11  CMS002_031a_Rb_S165_L004         493    114         13.261279\n",
       "12  CMS002_038a_Rb_S172_L004         493     22          8.685504"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viral_contam_reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about Baltimore classification of virus family groups\n",
    "viral_family_groups = pd.read_csv(\"../../data/virus_family_groups.csv\", header=0)\n",
    "viral_family_groups = viral_family_groups.loc[:, ~viral_family_groups.columns.str.startswith('Unnamed')]\n",
    "# Convert virus json into data frame\n",
    "with open (\"../../data/darkmatter/virus.json\", \"r\") as f:\n",
    "    viral_json = pd.DataFrame(json.load(f)).T\n",
    "    viral_json['poly_group'] = viral_json.index\n",
    "viral_contigs_df = pd.concat(viral_json.apply(get_viral_family_df, axis=1).tolist())\n",
    "numbers[\"num_viral_contigs\"] = len(viral_contigs_df)\n",
    "# Add read proportions columns\n",
    "viral_contigs = pd.merge(viral_contigs_df, contig_stats_all[[\"sample\", \"read_prop\", \"contig_name\"]], how=\"left\", on=[\"sample\", \"contig_name\"]).groupby([\"sample\", \"sci_name\", \"taxid\", \"poly_group\"])[\"read_prop\"].sum().reset_index()\n",
    "# Exclude viruses labelled as contamination\n",
    "viral_contigs = pd.merge(viral_contigs, viral_contam_reads[[\"sample\", \"poly_group\", \"background_reads\"]], how=\"left\", on=[\"sample\", \"poly_group\"])\n",
    "viral_contigs = viral_contigs[viral_contigs[\"background_reads\"].isnull()].drop(columns=\"background_reads\")\n",
    "# Add metadata information\n",
    "viral_contigs = pd.merge(viral_contigs, contig_stats_lca[[\"sample\"]+metadata_cols].groupby([\"sample\"]).first().reset_index(), how=\"left\")\n",
    "viral_contigs = pd.merge(viral_contigs, viral_contigs_df[[\"sample\", \"sci_name\", \"family\"]], how=\"left\")\n",
    "# Add baltimore group information about the viruses\n",
    "viral_contigs = pd.merge(viral_contigs, viral_family_groups, on=\"family\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curate lists of non-viral contigs with high-confidence hits to NCBI records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduct from the nonhost reads per sample the number of reads that were removed due to suspected contamination\n",
    "contig_stats_lca[\"nonhost_reads\"] = pd.merge(contig_stats_lca, contam_reads.groupby(\"sample\")[\"reads\"].sum().reset_index(), how=\"left\").apply(lambda x: x[\"nonhost_reads\"]-x[\"reads\"] if not math.isnan(x[\"reads\"]) else x[\"nonhost_reads\"], axis=1)\n",
    "contig_stats_lca[\"read_prop\"] = contig_stats_lca[\"read_count\"]/contig_stats_lca[\"nonhost_reads\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 197514.0 was translated into 2555385\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 257814.0 was translated into 2608709\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 2053043.0 was translated into 2665496\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 257814 was translated into 2608709\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/ete3/ncbi_taxonomy/ncbiquery.py:240: UserWarning: taxid 2053043 was translated into 2665496\n",
      "  warnings.warn(\"taxid %s was translated into %s\" %(taxid, merged_conversion[taxid]))\n"
     ]
    }
   ],
   "source": [
    "# Only keep hits that are almost identical to a known wolbachia sequence\n",
    "wolbachia_taxid = 952\n",
    "wolbachia_contigs = get_rows_taxid(contig_stats_lca, taxid=wolbachia_taxid, taxid_colname=\"taxid\", identity_qcov_cutoff=identity_qcov_threshold)\n",
    "# Only keep wolbachia groups that were not removed by the decontamination step\n",
    "wolbachia_contigs = pd.merge(wolbachia_contigs, true_reads, how=\"left\")\n",
    "wolbachia_contigs = wolbachia_contigs[~wolbachia_contigs[\"reads\"].isnull()]\n",
    "numbers[\"total_wolbachia_contigs\"] = len(wolbachia_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "wolbachia_contigs = get_summary_table(wolbachia_contigs, colnames=[\"ska_genus\", \"ska_species\", \"collected_by\", \"sample\", \"taxid\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# Correct read counts after decontamination\n",
    "wolbachia_contigs = correct_read_count(wolbachia_contigs, true_reads, wolbachia_taxid, \"taxid\", search_lower_lineages=True)\n",
    "# Create sci_name column to denote that that this table contains Wolbachia samples\n",
    "wolbachia_contigs = wolbachia_contigs.assign(sci_name=ncbi.get_taxid_translator([wolbachia_taxid])[wolbachia_taxid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep hits that are almost identical to a known metazoan sequence\n",
    "metazoan_contigs = contig_stats_lca[(contig_stats_lca[\"taxon_group\"]==\"Metazoa\") & (contig_stats_lca[\"identity_qcov\"]>=identity_qcov_threshold)]\n",
    "# Only keep metazoan groups that were not removed by the decontamination step\n",
    "metazoan_contigs = pd.merge(metazoan_contigs, true_reads, how=\"left\")\n",
    "metazoan_contigs = metazoan_contigs[~metazoan_contigs[\"reads\"].isnull()]\n",
    "# Convert taxids to those of interest\n",
    "#metazoan_contigs = clean_taxids(metazoan_contigs, taxids=[\"Leporidae\", \"Muroidea\", \"Homo sapiens\", \"Carnivora\", \"Odocoileinae\", \"Bovidae\", \"Neognathae\"], root_taxid=\"Metazoa\")\n",
    "metazoan_contigs = metazoan_contigs.assign(sci_name=metazoan_contigs[\"taxid\"].apply(lambda x: ncbi.get_taxid_translator([x])[x]))\n",
    "numbers[\"total_metazoan_contigs\"] = len(metazoan_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "metazoan_contigs = get_summary_table(metazoan_contigs, colnames=metadata_cols+[\"sample\", \"taxid\", \"sci_name\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# Correct read counts after decontamination\n",
    "metazoan_taxids = metazoan_contigs[\"taxid\"].unique().tolist()\n",
    "metazoan_contigs = correct_read_count(metazoan_contigs, true_reads, taxid=metazoan_taxids, taxid_colname=\"taxid\", search_lower_lineages=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep hits that are almost identical to a known chordate sequence\n",
    "# chordate_contigs = contig_stats_lca[(contig_stats_lca[\"taxon_group\"]==\"Metazoa\") & (contig_stats_lca[\"identity_qcov\"]>=identity_qcov_threshold)]\n",
    "chordate_contigs = get_rows_taxid(contig_stats_lca, taxid=\"Chordata\", taxid_colname=\"taxid\", identity_qcov_cutoff=identity_qcov_threshold)\n",
    "# Only keep metazoan groups that were not removed by the decontamination step\n",
    "chordate_contigs = pd.merge(chordate_contigs, true_reads, how=\"left\")\n",
    "chordate_contigs = chordate_contigs[~chordate_contigs[\"reads\"].isnull()]\n",
    "# Convert taxids to those of interest\n",
    "chordate_contigs = chordate_contigs.assign(sci_name=chordate_contigs[\"taxid\"].apply(lambda x: ncbi.get_taxid_translator([x])[x]))\n",
    "numbers[\"total_chordate_contigs\"] = len(chordate_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "chordate_contigs = get_summary_table(chordate_contigs, colnames=metadata_cols+[\"sample\", \"taxid\", \"sci_name\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# Correct read counts after decontamination\n",
    "chordate_taxids = chordate_contigs[\"taxid\"].unique().tolist()\n",
    "chordate_contigs = correct_read_count(chordate_contigs, true_reads, taxid=chordate_taxids, taxid_colname=\"taxid\", search_lower_lineages=False)\n",
    "# In the 'family' column, add information about the higher taxonomic grouping for taxids\n",
    "chordate_contigs = group_at_higher_tax(chordate_contigs, taxonomic_group=[\"Pecora\", \"Aves\", \"Carnivora\", \"Rodentia\", \"Leporidae\"], family_name=\"Other chordates\", taxid_colname=\"taxid\", family_colname=\"family\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep hits that are almost identical to a known eukaryote sequence\n",
    "eukaryotic_contigs_df = contig_stats_lca[(contig_stats_lca[\"taxon_group\"]==\"Eukaryota\") & (contig_stats_lca[\"identity_qcov\"]>=identity_qcov_threshold)]\n",
    "# Only keep eukaryotic groups that were not removed by the decontamination step\n",
    "eukaryotic_contigs_df = pd.merge(eukaryotic_contigs_df, true_reads, how=\"left\")\n",
    "eukaryotic_contigs_df = eukaryotic_contigs_df[~eukaryotic_contigs_df[\"reads\"].isnull()]\n",
    "# Convert taxids to those of interest\n",
    "eukaryotic_contigs = eukaryotic_contigs_df.assign(sci_name=eukaryotic_contigs_df[\"taxid\"].apply(lambda x: ncbi.get_taxid_translator([x])[x]))\n",
    "numbers[\"total_eukaryotic_contigs\"] = len(eukaryotic_contigs)\n",
    "# Return a list of species grouped by mosquito species, collection site, sample, and taxid, and sorted by total read count\n",
    "eukaryotic_contigs = get_summary_table(eukaryotic_contigs, colnames=metadata_cols+[\"sample\", \"taxid\", \"sci_name\"], metric=[\"read_count\", \"read_prop\"])\n",
    "# Correct read counts after decontamination\n",
    "eukaryotic_taxids = eukaryotic_contigs[\"taxid\"].unique().tolist()\n",
    "eukaryotic_contigs = correct_read_count(eukaryotic_contigs, true_reads, taxid=eukaryotic_taxids, taxid_colname=\"taxid\", search_lower_lineages=False) \n",
    "# In the 'family' column, add information about the higher taxonomic grouping for taxids\n",
    "eukaryotic_contigs = group_at_higher_tax(eukaryotic_contigs, taxonomic_group=[\"Trypanosomatidae\", \"Apicomplexa\", \"Amblyosporidae\"], family_name=\"Other eukaryotes\", taxid_colname=\"taxid\", family_colname=\"family\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-4424c5911405>:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  all_contigs_df = pd.concat([wolbachia_contigs.assign(group=\"Wolbachia\"),\n"
     ]
    }
   ],
   "source": [
    "all_contigs_df = pd.concat([wolbachia_contigs.assign(group=\"Wolbachia\"), \n",
    "                            viral_contigs.assign(group=\"Virus\"),\n",
    "                            metazoan_contigs.assign(group=\"Metazoa\"), \n",
    "                            eukaryotic_contigs.assign(group=\"Other Eukaryotes\")])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_contigs_df.to_csv(\"../../figures/fig3/all_contigs_df.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hexapoda_decontam_reads = contig_stats_lca[[\"sample\", \"nonhost_reads\"]][(~contig_stats_lca[\"sample\"].duplicated()) & (~contig_stats_lca[\"nonhost_reads\"].isnull())]\n",
    "hexapoda_decontam_reads = hexapoda_decontam_reads.astype({\"nonhost_reads\":int})\n",
    "hexapoda_decontam_reads.to_csv(\"../../data/metadata/nonhost_reads_decontam_nohexapoda.tsv\", index=False, sep=\"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
