{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from ete3 import NCBITaxa\n",
    "import subprocess\n",
    "import itertools\n",
    "import os\n",
    "import s3fs\n",
    "import numpy as np\n",
    "from lca_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "client = boto3.client('s3')\n",
    "bucket_name = \"czbiohub-mosquito\"\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "contig_folders = [x[\"Prefix\"] for x in client.list_objects(Bucket=bucket_name, Prefix=\"contigs/\", Delimiter=\"/\")[\"CommonPrefixes\"]]\n",
    "contig_quality_folders = [x[\"Prefix\"] for x in client.list_objects(Bucket=bucket_name, Prefix=\"contig_quality/\", Delimiter=\"/\")[\"CommonPrefixes\"] if \"Mos\" not in x[\"Prefix\"]]\n",
    "\n",
    "ncores = os.cpu_count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  new_taxdump_2019-01-01.zip\n",
      "  inflating: new_taxdump_2019-01-01/citations.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/delnodes.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/division.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/fullnamelineage.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/gencode.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/host.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/merged.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/names.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/nodes.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/rankedlineage.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/taxidlineage.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/typematerial.dmp  \n",
      "  inflating: new_taxdump_2019-01-01/typeoftype.dmp  \n",
      "Archive:  new_taxdump_2019-06-01.zip\n",
      "  inflating: new_taxdump_2019-06-01/citations.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/delnodes.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/division.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/fullnamelineage.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/gencode.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/host.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/merged.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/names.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/nodes.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/rankedlineage.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/taxidlineage.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/typematerial.dmp  \n",
      "  inflating: new_taxdump_2019-06-01/typeoftype.dmp  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0 96.0M    0  231k    0     0   157k      0  0:10:25  0:00:01  0:10:24  157k\r",
      "  4 96.0M    4 4181k    0     0  1674k      0  0:00:58  0:00:02  0:00:56 1673k\r",
      "  9 96.0M    9 9332k    0     0  2683k      0  0:00:36  0:00:03  0:00:33 2682k\r",
      " 16 96.0M   16 16.0M    0     0  3677k      0  0:00:26  0:00:04  0:00:22 3676k\r",
      " 25 96.0M   25 24.7M    0     0  4594k      0  0:00:21  0:00:05  0:00:16 5066k\r",
      " 33 96.0M   33 31.9M    0     0  5040k      0  0:00:19  0:00:06  0:00:13 6472k\r",
      " 39 96.0M   39 38.4M    0     0  5261k      0  0:00:18  0:00:07  0:00:11 7061k\r",
      " 48 96.0M   48 46.2M    0     0  5591k      0  0:00:17  0:00:08  0:00:09 7615k\r",
      " 57 96.0M   57 55.2M    0     0  5975k      0  0:00:16  0:00:09  0:00:07 8034k\r",
      " 63 96.0M   63 61.1M    0     0  5979k      0  0:00:16  0:00:10  0:00:06 7517k\r",
      " 70 96.0M   70 67.4M    0     0  6015k      0  0:00:16  0:00:11  0:00:05 7288k\r",
      " 78 96.0M   78 75.1M    0     0  6167k      0  0:00:15  0:00:12  0:00:03 7519k\r",
      " 88 96.0M   88 85.4M    0     0  6492k      0  0:00:15  0:00:13  0:00:02 8018k\r",
      "100 96.0M  100 96.0M    0     0  6824k      0  0:00:14  0:00:14 --:--:-- 8455k\n",
      "a citations.dmp\n",
      "a delnodes.dmp\n",
      "a division.dmp\n",
      "a fullnamelineage.dmp\n",
      "a gencode.dmp\n",
      "a host.dmp\n",
      "a merged.dmp\n",
      "a names.dmp\n",
      "a nodes.dmp\n",
      "a rankedlineage.dmp\n",
      "a taxidlineage.dmp\n",
      "a typematerial.dmp\n",
      "a typeoftype.dmp\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0 99.4M    0  173k    0     0   107k      0  0:15:47  0:00:01  0:15:46  107k\r",
      "  6 99.4M    6 6228k    0     0  2375k      0  0:00:42  0:00:02  0:00:40 2374k\r",
      " 14 99.4M   14 14.0M    0     0  4013k      0  0:00:25  0:00:03  0:00:22 4012k\r",
      " 24 99.4M   24 24.3M    0     0  5430k      0  0:00:18  0:00:04  0:00:14 5429k\r",
      " 37 99.4M   37 37.3M    0     0  6844k      0  0:00:14  0:00:05  0:00:09 7683k\r",
      " 48 99.4M   48 48.1M    0     0  7489k      0  0:00:13  0:00:06  0:00:07 9893k\r",
      " 56 99.4M   56 56.4M    0     0  7610k      0  0:00:13  0:00:07  0:00:06 10.1M\r",
      " 66 99.4M   66 65.7M    0     0  7844k      0  0:00:12  0:00:08  0:00:04 10.3M\r",
      " 76 99.4M   76 75.6M    0     0  8075k      0  0:00:12  0:00:09  0:00:03 10.2M\r",
      " 82 99.4M   82 82.0M    0     0  7913k      0  0:00:12  0:00:10  0:00:02 9101k\r",
      " 89 99.4M   89 88.6M    0     0  7829k      0  0:00:13  0:00:11  0:00:02 8277k\r",
      " 97 99.4M   97 96.7M    0     0  7870k      0  0:00:12  0:00:12 --:--:-- 8264k\r",
      "100 99.4M  100 99.4M    0     0  7869k      0  0:00:12  0:00:12 --:--:-- 7919k\n",
      "a citations.dmp\n",
      "a delnodes.dmp\n",
      "a division.dmp\n",
      "a fullnamelineage.dmp\n",
      "a gencode.dmp\n",
      "a host.dmp\n",
      "a merged.dmp\n",
      "a names.dmp\n",
      "a nodes.dmp\n",
      "a rankedlineage.dmp\n",
      "a taxidlineage.dmp\n",
      "a typematerial.dmp\n",
      "a typeoftype.dmp\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "download_file() {\n",
    "    tax_db=$1\n",
    "    if [ ! -f \"${tax_db}.zip\" ]; then\n",
    "        curl -O ftp://ftp.ncbi.nlm.nih.gov/pub/taxonomy/taxdump_archive/$tax_db.zip\n",
    "        unzip -d $tax_db $tax_db.zip\n",
    "        cd $tax_db\n",
    "        tar -czvf ../$tax_db.tar.gz *\n",
    "        cd ..\n",
    "        rm -rf $tax_db $tax_db.zip\n",
    "    fi\n",
    "}\n",
    "export -f download_file\n",
    "download_file \"new_taxdump_2019-01-01\" ## latest taxonomy release that still contains original virus taxonomy\n",
    "download_file \"new_taxdump_2019-06-01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncbi = NCBITaxa()\n",
    "#ncbi.update_taxonomy_database(taxdump_file==\"new_taxdump_2019-06-01.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lca_analysis (input_file_name, output_dir, bucket_name, blast_type, default=False, ncores=8):\n",
    "    # First list the folders (there is a limit of 1000 files output by AWS by default)\n",
    "    list_of_folders = [client.list_objects(Bucket=bucket_name, Prefix=x[\"Prefix\"]+input_file_name) \\\n",
    "                       for x in client.list_objects(Bucket=bucket_name, Prefix=\"contigs/\", Delimiter=\"/\")[\"CommonPrefixes\"]]\n",
    "    list_of_folders = [\"s3://\"+bucket_name+\"/\"+x[\"Prefix\"] for x in list_of_folders if \"Contents\" in x.keys()]\n",
    "    filenames = pd.DataFrame(list_of_folders, columns=[\"blast_\"+blast_type])\n",
    "    output_string = output_dir\n",
    "    if not default:\n",
    "        output_string += \"/ident\"+str(ident_cutoff)+\"align\"+str(align_cutoff)+\"bitscore\"+str(bitscore_cutoff)\n",
    "    filenames = filenames.assign(filtered_blast=filenames[\"blast_\"+blast_type].str.replace(\"contigs\", output_string).str.replace(\".m9\", \"_filtered.m9\"))\n",
    "    filenames = filenames.assign(excluded_contigs=filenames[\"filtered_blast\"].apply(os.path.dirname).apply(lambda x: os.path.join(x, \"exclude_contigs_\"+blast_type+\".txt\")))\n",
    "    filenames = filenames.assign(lca=filenames[\"filtered_blast\"].str.replace(\"blast_\"+blast_type, \"lca_\"+blast_type).str.replace(\"_filtered\", \"\"))\n",
    "    filenames = filenames.assign(reads=filenames[\"blast_\"+blast_type].str.replace(\"blast_\"+blast_type+\".m9\", \"contig_stats.json\").replace(\"\"))\n",
    "    #filenames.loc[~filenames[\"reads\"].str.contains(\"ater\"), \"reads\"] = filenames.loc[~filenames[\"reads\"].str.contains(\"ater\"), \"reads\"].str.replace(\"bowtie\", \"Mos/bowtie\")\n",
    "    commands = filenames.apply(lambda x: \"python lca_analysis.py\"+\\\n",
    "                               \" --blast_type \"+blast_type+\\\n",
    "                               \" --fpath \"+x.iloc[0]+\\\n",
    "                               \" --filtered_blast_path \"+x.iloc[1]+\\\n",
    "                               \" --excluded_contigs_path \"+x.iloc[2]+\\\n",
    "                               \" --outpath \"+x.iloc[3]+\\\n",
    "                               \" --read_count_path \"+x.iloc[4]+\\\n",
    "                               \" --verbose True\", axis=1)\n",
    "    print (commands)\n",
    "    commands_csv_filename = \"lca_\"+blast_type+\"_commands\"\n",
    "    commands.to_csv(commands_csv_filename, index=False)\n",
    "    if (len(filenames) < ncores):\n",
    "        ncores = len(filenames)\n",
    "    command_str = \"parallel -a \"+commands_csv_filename+\" -j \"+str(ncores)\n",
    "    print (command_str)\n",
    "#     process = subprocess.Popen(command_str.split(), stdout=subprocess.PIPE)\n",
    "#     output, error = process.communicate()\n",
    "#     return (output, error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nt hits\n",
    "The nt hits of contigs from each sample are filtered with ident_cutoff=0.9 and align_len_cutoff=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      python lca_analysis.py --blast_type nt --fpath...\n",
      "1      python lca_analysis.py --blast_type nt --fpath...\n",
      "2      python lca_analysis.py --blast_type nt --fpath...\n",
      "3      python lca_analysis.py --blast_type nt --fpath...\n",
      "4      python lca_analysis.py --blast_type nt --fpath...\n",
      "5      python lca_analysis.py --blast_type nt --fpath...\n",
      "6      python lca_analysis.py --blast_type nt --fpath...\n",
      "7      python lca_analysis.py --blast_type nt --fpath...\n",
      "8      python lca_analysis.py --blast_type nt --fpath...\n",
      "9      python lca_analysis.py --blast_type nt --fpath...\n",
      "10     python lca_analysis.py --blast_type nt --fpath...\n",
      "11     python lca_analysis.py --blast_type nt --fpath...\n",
      "12     python lca_analysis.py --blast_type nt --fpath...\n",
      "13     python lca_analysis.py --blast_type nt --fpath...\n",
      "14     python lca_analysis.py --blast_type nt --fpath...\n",
      "15     python lca_analysis.py --blast_type nt --fpath...\n",
      "16     python lca_analysis.py --blast_type nt --fpath...\n",
      "17     python lca_analysis.py --blast_type nt --fpath...\n",
      "18     python lca_analysis.py --blast_type nt --fpath...\n",
      "19     python lca_analysis.py --blast_type nt --fpath...\n",
      "20     python lca_analysis.py --blast_type nt --fpath...\n",
      "21     python lca_analysis.py --blast_type nt --fpath...\n",
      "22     python lca_analysis.py --blast_type nt --fpath...\n",
      "23     python lca_analysis.py --blast_type nt --fpath...\n",
      "24     python lca_analysis.py --blast_type nt --fpath...\n",
      "25     python lca_analysis.py --blast_type nt --fpath...\n",
      "26     python lca_analysis.py --blast_type nt --fpath...\n",
      "27     python lca_analysis.py --blast_type nt --fpath...\n",
      "28     python lca_analysis.py --blast_type nt --fpath...\n",
      "29     python lca_analysis.py --blast_type nt --fpath...\n",
      "                             ...                        \n",
      "125    python lca_analysis.py --blast_type nt --fpath...\n",
      "126    python lca_analysis.py --blast_type nt --fpath...\n",
      "127    python lca_analysis.py --blast_type nt --fpath...\n",
      "128    python lca_analysis.py --blast_type nt --fpath...\n",
      "129    python lca_analysis.py --blast_type nt --fpath...\n",
      "130    python lca_analysis.py --blast_type nt --fpath...\n",
      "131    python lca_analysis.py --blast_type nt --fpath...\n",
      "132    python lca_analysis.py --blast_type nt --fpath...\n",
      "133    python lca_analysis.py --blast_type nt --fpath...\n",
      "134    python lca_analysis.py --blast_type nt --fpath...\n",
      "135    python lca_analysis.py --blast_type nt --fpath...\n",
      "136    python lca_analysis.py --blast_type nt --fpath...\n",
      "137    python lca_analysis.py --blast_type nt --fpath...\n",
      "138    python lca_analysis.py --blast_type nt --fpath...\n",
      "139    python lca_analysis.py --blast_type nt --fpath...\n",
      "140    python lca_analysis.py --blast_type nt --fpath...\n",
      "141    python lca_analysis.py --blast_type nt --fpath...\n",
      "142    python lca_analysis.py --blast_type nt --fpath...\n",
      "143    python lca_analysis.py --blast_type nt --fpath...\n",
      "144    python lca_analysis.py --blast_type nt --fpath...\n",
      "145    python lca_analysis.py --blast_type nt --fpath...\n",
      "146    python lca_analysis.py --blast_type nt --fpath...\n",
      "147    python lca_analysis.py --blast_type nt --fpath...\n",
      "148    python lca_analysis.py --blast_type nt --fpath...\n",
      "149    python lca_analysis.py --blast_type nt --fpath...\n",
      "150    python lca_analysis.py --blast_type nt --fpath...\n",
      "151    python lca_analysis.py --blast_type nt --fpath...\n",
      "152    python lca_analysis.py --blast_type nt --fpath...\n",
      "153    python lca_analysis.py --blast_type nt --fpath...\n",
      "154    python lca_analysis.py --blast_type nt --fpath...\n",
      "Length: 155, dtype: object\n",
      "parallel -a lca_nt_commands -j 8\n"
     ]
    }
   ],
   "source": [
    "run_lca_analysis(input_file_name=\"blast_nt.m9\", output_dir=\"contig_quality\", \\\n",
    "                 bucket_name=bucket_name, blast_type=\"nt\", default=True, ncores=ncores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lca_nt_paths = [\"s3://\"+bucket_name+\"/\"+x[\"Prefix\"]+\"lca_nt.m9\" \\\n",
    " for x in client.list_objects(Bucket=bucket_name, Prefix=\"contig_quality/\", Delimiter=\"/\")[\"CommonPrefixes\"] if \"Mos\" not in x[\"Prefix\"]]\n",
    "blast_nt_paths = [x.replace(\"lca_nt\", \"blast_nt_filtered\") for x in lca_nt_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: CMS001_Water5_RNA_A_S12\n",
      "error: CMS001_water1_S11\n",
      "error: CMS001_water5_RNA_A_S12\n",
      "error: CMS002_016a_Rb_S121_L004\n",
      "error: CMS002_025d_Rb_S143_L004\n",
      "error: CMS002_025f_Rb_S145_L004\n",
      "error: CMS002_0Water8_Rb_S11_L004\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lca_nt_paths)):\n",
    "    sample_name = os.path.basename(os.path.dirname(lca_nt_paths[i]))\n",
    "    outfile = lca_nt_paths[i].replace(\"lca_nt\", \"blast_lca_nt_filtered\")\n",
    "    try:\n",
    "        combine_blast_lca (lca_nt_paths[i], blast_nt_paths[i], outfile, sample_name, \"nt\")\n",
    "    except:\n",
    "        print (\"error: \"+sample_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nr hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract blast_nr hits from plast results and save to folder for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "aws s3 ls s3://czbiohub-mosquito/plast/ | grep '.m8' | awk 'NF>1{print $NF}' | parallel -j 72 python create_blast_nr.py --fpath s3://czbiohub-mosquito/plast/{}\n",
    "aws s3 ls s3://lucymli/skeeters/blast_nr/ | grep 'CMS00' | awk 'NF>1{print $NF}' | parallel aws s3 sync s3://lucymli/skeeters/blast_nr/{} blast_nr_output/{}\n",
    "head -n 1 $(find blast_nr_output -type f -name '*.m8' | head -n 1) > header_line \n",
    "for x in `ls blast_nr_output`; do \n",
    "    mkdir -p blast_nr_output_full/$x\n",
    "    head -n 1 $(find blast_nr_output -type f -name '*.m8' | head -n 1) > blast_nr_output_full/$x/blast_nr.m9\n",
    "    ls -d $(find blast_nr_output/$x -type f) | xargs -0 -I file cat file > blast_nr_output_full/$x/blast_nr.m9\n",
    "done\n",
    "ls blast_nr_output_full | parallel aws s3 cp blast_nr_output_full/{}/blast_nr.m9 s3://czbiohub-mosquito/contigs/{}/blast_nr.m9\n",
    "\n",
    "# add missing taxids\n",
    "python initiate_gi2taxid_database.py\n",
    "aws s3 ls s3://czbiohub-mosquito/contigs/ --recursive | grep \"blast_nr.m9\" | awk 'NF>1{print $NF}' | parallel python get_missing_prot_taxa.py s3://czbiohub-mosquito/{}\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nr hits of contigs from each sample are filtered with ident_cutoff=0.9 and align_len_cutoff=0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      python lca_analysis.py --blast_type nr --fpath...\n",
      "1      python lca_analysis.py --blast_type nr --fpath...\n",
      "2      python lca_analysis.py --blast_type nr --fpath...\n",
      "3      python lca_analysis.py --blast_type nr --fpath...\n",
      "4      python lca_analysis.py --blast_type nr --fpath...\n",
      "                             ...                        \n",
      "150    python lca_analysis.py --blast_type nr --fpath...\n",
      "151    python lca_analysis.py --blast_type nr --fpath...\n",
      "152    python lca_analysis.py --blast_type nr --fpath...\n",
      "153    python lca_analysis.py --blast_type nr --fpath...\n",
      "154    python lca_analysis.py --blast_type nr --fpath...\n",
      "Length: 155, dtype: object\n",
      "parallel -a lca_nr_commands -j 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucy/anaconda3/envs/skeeters/lib/python3.7/site-packages/ipykernel_launcher.py:31: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n"
     ]
    }
   ],
   "source": [
    "run_lca_analysis(input_file_name=\"blast_nr.m9\", output_dir=\"contig_quality\", \\\n",
    "                 bucket_name=bucket_name, blast_type=\"nr\", default=True, ncores=ncores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lca_nr_paths = [\"s3://\"+bucket_name+\"/\"+x[\"Prefix\"]+\"lca_nr.m9\" \\\n",
    " for x in client.list_objects(Bucket=bucket_name, Prefix=\"contig_quality/\", Delimiter=\"/\")[\"CommonPrefixes\"] if \"Mos\" not in x[\"Prefix\"]]\n",
    "blast_nr_paths = [x.replace(\"lca_nr\", \"blast_nr_filtered\") for x in lca_nr_paths]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lca_nr_paths)):\n",
    "    sample_name = os.path.basename(os.path.dirname(lca_nr_paths[i]))\n",
    "    outfile = lca_nr_paths[i].replace(\"lca_nr\", \"blast_lca_nr_filtered\")\n",
    "    try:\n",
    "        combine_blast_lca (lca_nr_paths[i], blast_nr_paths[i], outfile, sample_name, \"nr\")\n",
    "    except:\n",
    "        print (\"error: \"+sample_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_type=\"nt\"\n",
    "fpath=\"s3://czbiohub-mosquito/contigs/CMS002_038a_Rb_S172_L004/blast_nt.m9\"\n",
    "filtered_blast_path=\"s3://czbiohub-mosquito/contig_quality/CMS002_038a_Rb_S172_L004/blast_nt_filtered.m9\" \n",
    "excluded_contigs_path=\"s3://czbiohub-mosquito/contig_quality/CMS002_038a_Rb_S172_L004/exclude_contigs_nt.txt\"\n",
    "outpath=\"s3://czbiohub-mosquito/contig_quality/CMS002_038a_Rb_S172_L004/lca_nt.m9\" \n",
    "read_count_path=\"s3://czbiohub-mosquito/contigs/CMS002_038a_Rb_S172_L004/contig_stats.json\"\n",
    "verbose=True\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, sys, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (read_count_path.endswith(\".json\")):\n",
    "    read_counts = load_json(read_count_path, colnames=[\"query\", \"read_count\"])\n",
    "else:\n",
    "    read_counts = pd.read_csv(read_count_path, sep=\"\\t\", header=0).rename(columns={\"contig_name\":\"query\"})\n",
    "\n",
    "filtered_contigs_by_read_count = read_counts[read_counts[\"read_count\"]>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_names = [\"query\", \"subject\", \"identity\", \"align_length\", \"mismatches\", \n",
    "        \"gaps\", \"qstart\", \"qend\", \"sstart\", \"send\", \"evalue\", \"bitscore\", \"taxid\", \n",
    "        \"sci_name\", \"common_name\", \"subject_title\", \"qcov\", \"hsp_count\"]\n",
    "\n",
    "db = \"nucleotide\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/82/tl56r6r132sbl5nnqn9jpytc0000gn/T/tmppnr6fi3f blast file downloaded to this tempfile\n"
     ]
    }
   ],
   "source": [
    "blast_results = get_single_hsp(fpath, blast_type, col_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>subject</th>\n",
       "      <th>identity</th>\n",
       "      <th>align_length</th>\n",
       "      <th>mismatches</th>\n",
       "      <th>gaps</th>\n",
       "      <th>qstart</th>\n",
       "      <th>qend</th>\n",
       "      <th>sstart</th>\n",
       "      <th>send</th>\n",
       "      <th>evalue</th>\n",
       "      <th>bitscore</th>\n",
       "      <th>taxid</th>\n",
       "      <th>sci_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>subject_title</th>\n",
       "      <th>qcov</th>\n",
       "      <th>hsp_count</th>\n",
       "      <th>blast_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NODE_7_length_3154_cov_2211.364316</td>\n",
       "      <td>KX882764.1</td>\n",
       "      <td>81.250030</td>\n",
       "      <td>2320</td>\n",
       "      <td>252</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1537</td>\n",
       "      <td>3077</td>\n",
       "      <td>1555</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1856.0</td>\n",
       "      <td>1922926</td>\n",
       "      <td>Hubei mosquito virus 2</td>\n",
       "      <td>Hubei mosquito virus 2</td>\n",
       "      <td>Hubei mosquito virus 2 strain 3mos6212 segment...</td>\n",
       "      <td>0.735574</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NODE_7_length_3154_cov_2211.364316</td>\n",
       "      <td>KX882873.1</td>\n",
       "      <td>81.034207</td>\n",
       "      <td>2320</td>\n",
       "      <td>255</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1537</td>\n",
       "      <td>3072</td>\n",
       "      <td>1550</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1827.0</td>\n",
       "      <td>1922926</td>\n",
       "      <td>Hubei mosquito virus 2</td>\n",
       "      <td>Hubei mosquito virus 2</td>\n",
       "      <td>Hubei mosquito virus 2 strain spider133708 seg...</td>\n",
       "      <td>0.735574</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NODE_7_length_3154_cov_2211.364316</td>\n",
       "      <td>KX882832.1</td>\n",
       "      <td>79.148709</td>\n",
       "      <td>2326</td>\n",
       "      <td>292</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>1537</td>\n",
       "      <td>3082</td>\n",
       "      <td>1558</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1584.0</td>\n",
       "      <td>1922926</td>\n",
       "      <td>Hubei mosquito virus 2</td>\n",
       "      <td>Hubei mosquito virus 2</td>\n",
       "      <td>Hubei mosquito virus 2 strain mosZJ35453 segme...</td>\n",
       "      <td>0.737476</td>\n",
       "      <td>2.0</td>\n",
       "      <td>nt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NODE_7_length_3154_cov_2211.364316</td>\n",
       "      <td>MH188027.1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2129</td>\n",
       "      <td>2158</td>\n",
       "      <td>4816</td>\n",
       "      <td>4787</td>\n",
       "      <td>0.006</td>\n",
       "      <td>56.5</td>\n",
       "      <td>2304509</td>\n",
       "      <td>Culex Daeseongdong-like virus</td>\n",
       "      <td>Culex Daeseongdong-like virus</td>\n",
       "      <td>Culex Daeseongdong-like virus strain CDaeVL/Ke...</td>\n",
       "      <td>0.009512</td>\n",
       "      <td>1.0</td>\n",
       "      <td>nt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 query     subject    identity  align_length  \\\n",
       "43  NODE_7_length_3154_cov_2211.364316  KX882764.1   81.250030          2320   \n",
       "44  NODE_7_length_3154_cov_2211.364316  KX882873.1   81.034207          2320   \n",
       "45  NODE_7_length_3154_cov_2211.364316  KX882832.1   79.148709          2326   \n",
       "46  NODE_7_length_3154_cov_2211.364316  MH188027.1  100.000000            30   \n",
       "\n",
       "    mismatches  gaps  qstart  qend  sstart  send  evalue  bitscore    taxid  \\\n",
       "43         252     6      15  1537    3077  1555   0.000    1856.0  1922926   \n",
       "44         255     6      15  1537    3072  1550   0.000    1827.0  1922926   \n",
       "45         292    14      13  1537    3082  1558   0.000    1584.0  1922926   \n",
       "46           0     0    2129  2158    4816  4787   0.006      56.5  2304509   \n",
       "\n",
       "                         sci_name                    common_name  \\\n",
       "43         Hubei mosquito virus 2         Hubei mosquito virus 2   \n",
       "44         Hubei mosquito virus 2         Hubei mosquito virus 2   \n",
       "45         Hubei mosquito virus 2         Hubei mosquito virus 2   \n",
       "46  Culex Daeseongdong-like virus  Culex Daeseongdong-like virus   \n",
       "\n",
       "                                        subject_title      qcov  hsp_count  \\\n",
       "43  Hubei mosquito virus 2 strain 3mos6212 segment...  0.735574        2.0   \n",
       "44  Hubei mosquito virus 2 strain spider133708 seg...  0.735574        2.0   \n",
       "45  Hubei mosquito virus 2 strain mosZJ35453 segme...  0.737476        2.0   \n",
       "46  Culex Daeseongdong-like virus strain CDaeVL/Ke...  0.009512        1.0   \n",
       "\n",
       "   blast_type  \n",
       "43         nt  \n",
       "44         nt  \n",
       "45         nt  \n",
       "46         nt  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blast_results[blast_results[\"query\"]==\"NODE_7_length_3154_cov_2211.364316\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/folders/82/tl56r6r132sbl5nnqn9jpytc0000gn/T/tmp0w1kzbfg blast file downloaded to this tempfile\n",
      "Loaded blast file: s3://czbiohub-mosquito/contigs/CMS002_038a_Rb_S172_L004/blast_nt.m9| elapsed time: 18.27 seconds\n",
      "0 contigs were excluded because none of the subject taxids could be found.| elapsed time: 18.29 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# obtain a single HSP for each query-subject pairing and read in blast results as a pandas data frame\n",
    "blast_results = get_single_hsp(fpath, blast_type, col_names) \n",
    "\n",
    "if \"qlen\" not in blast_results:\n",
    "    if (\"~\" in blast_results[\"query\"].iloc[0]):\n",
    "        blast_results = blast_results.assign(qlen=blast_results[\"query\"].str.split(\"~\").apply(lambda x: int(x[1].split(\"_\")[3])))\n",
    "    else:\n",
    "        blast_results = blast_results.assign(qlen=blast_results[\"query\"].str.split(\"_\").apply(lambda x: int(x[3])))\n",
    "print_to_stdout(\"Loaded blast file: \"+fpath, start_time, verbose)\n",
    "\n",
    "\n",
    "# data frame: whether or not each contig was included or excluded from blast analysis, and reason for exclusion\n",
    "excluded_contigs = blast_results.groupby([\"query\"]).first().reset_index()[[\"query\", \"qlen\"]].rename(columns={\"qlen\":\"contig_length\"})\n",
    "if (\"~\" in excluded_contigs[\"query\"].iloc[0]):\n",
    "    excluded_contigs = excluded_contigs.assign(sample=excluded_contigs[\"query\"].str.split(\"~\").apply(lambda x: x[0]))\n",
    "    excluded_contigs[\"query\"] = excluded_contigs[\"query\"].str.split(\"~\").apply(lambda x: x[1])\n",
    "    if \"sample\" in read_counts:\n",
    "        selected_cols = [\"sample\", \"query\", \"read_count\"]\n",
    "    else:\n",
    "        selected_cols = [\"query\", \"read_count\"]\n",
    "    excluded_contigs = pd.merge(excluded_contigs, read_counts[selected_cols], how=\"left\").fillna(0)\n",
    "    excluded_contigs[\"query\"] = excluded_contigs[[\"sample\", \"query\"]].apply(lambda x: x[0]+\"~\"+x[1], axis=1)\n",
    "    queries = filtered_contigs_by_read_count.apply(lambda x: x[\"sample\"]+\"~\"+x[\"query\"], axis=1)\n",
    "    excluded_contigs = excluded_contigs.assign(low_read_count=~excluded_contigs[\"query\"].isin(queries))\n",
    "else:\n",
    "    excluded_contigs = excluded_contigs.assign(contig_length=excluded_contigs[\"query\"].str.split(\"_\").apply(lambda x: int(x[3])))\n",
    "    excluded_contigs = pd.merge(excluded_contigs, read_counts, how=\"left\", on=\"query\").fillna(0)\n",
    "    excluded_contigs = excluded_contigs.assign(low_read_count=~excluded_contigs[\"query\"].isin(filtered_contigs_by_read_count[\"query\"]))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# find missing taxids\n",
    "blast_results[\"taxid\"] = blast_results[\"taxid\"].replace(to_replace=0, value=np.nan) # some synthetic constructs have taxid 0\n",
    "if (blast_results[\"taxid\"].isnull().any()):\n",
    "    subjects_to_search = list(blast_results[blast_results[\"taxid\"].isnull()][\"subject\"].unique())\n",
    "    print_to_stdout(str(blast_results[\"taxid\"].isnull().sum())+\" blast hits corresponding to \"+str(len(subjects_to_search))+\" accession numbers have taxid 'NA'. Trying to find the taxid for these hits on NCBI.\", start_time, verbose)\n",
    "    subjects_taxids = [find_missing_taxid(x, db=db) for x in subjects_to_search]\n",
    "    subjects_taxid_dict = dict(zip(subjects_to_search, subjects_taxids))\n",
    "    blast_results.loc[blast_results[\"taxid\"].isnull(), [\"taxid\"]] = blast_results[blast_results[\"taxid\"].isnull()][\"subject\"].apply(lambda x: subjects_taxid_dict[x])\n",
    "    blast_results = blast_results[~blast_results[\"taxid\"].isnull()]\n",
    "\n",
    "    \n",
    "excluded_contigs = excluded_contigs.assign(taxid_na=~excluded_contigs[\"query\"].isin(blast_results[\"query\"]))\n",
    "print_to_stdout(str(excluded_contigs[\"taxid_na\"].sum())+\" contigs were excluded because none of the subject taxids could be found.\", start_time, verbose)        \n",
    "\n",
    "if len(blast_results)==0:\n",
    "    if (excluded_contigs_path.startswith(\"s3://\")):\n",
    "        df_to_s3(excluded_contigs, excluded_contigs_path)\n",
    "    else:\n",
    "        excluded_contigs.to_csv(excluded_contigs_path, sep=\"\\t\", index=False)\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove contigs if they are likely hexapoda | elapsed time: 43.65 seconds\n"
     ]
    }
   ],
   "source": [
    "# exclude contigs with hits to mosquito\n",
    "all_hits_queries = list(blast_results[\"query\"].unique())\n",
    "print_to_stdout(\"remove contigs if they are likely hexapoda \", start_time, verbose)\n",
    "subset_blast_hits = blast_results[~blast_results[\"taxid\"].duplicated()]\n",
    "hexapoda_hits = ncbi.get_descendant_taxa(ncbi.get_name_translator([\"Hexapoda\"])[\"Hexapoda\"][0])\n",
    "hexapoda_queries = subset_blast_hits[subset_blast_hits[\"taxid\"].isin(hexapoda_hits)][\"query\"].unique().tolist()\n",
    "before = blast_results[blast_results[\"query\"].isin(hexapoda_queries)]\n",
    "after = before.groupby([\"query\"], as_index=False).apply(filter_by_taxid, db=db, taxid=ncbi_older_db([\"Hexapoda\"], \"get_name_translator\")[\"Hexapoda\"][0])\n",
    "hexa_contigs = before[~before[\"query\"].isin(after[\"query\"])][\"query\"].unique()\n",
    "blast_results = blast_results[~blast_results[\"query\"].isin(hexa_contigs)]\n",
    "excluded_contigs = excluded_contigs.assign(hexapoda=excluded_contigs[\"query\"].isin(hexa_contigs))\n",
    "print_to_stdout(str(len(hexa_contigs))+\" contigs were likely hexapoda.\", start_time, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NODE_57_length_1623_cov_3.124838',\n",
       " 'NODE_66_length_1522_cov_1.734256',\n",
       " 'NODE_81_length_1424_cov_11.271715',\n",
       " 'NODE_100_length_1330_cov_4.233041',\n",
       " 'NODE_110_length_1279_cov_1.802829',\n",
       " 'NODE_117_length_1264_cov_1.492839',\n",
       " 'NODE_178_length_1128_cov_3.803996',\n",
       " 'NODE_294_length_955_cov_2.523918',\n",
       " 'NODE_372_length_858_cov_2.294494',\n",
       " 'NODE_687_length_677_cov_1.213333',\n",
       " 'NODE_777_length_635_cov_3.609319',\n",
       " 'NODE_792_length_630_cov_0.989150',\n",
       " 'NODE_867_length_609_cov_1.592105',\n",
       " 'NODE_1095_length_554_cov_0.909853',\n",
       " 'NODE_1118_length_546_cov_299.526652',\n",
       " 'NODE_1168_length_537_cov_1.715217',\n",
       " 'NODE_1218_length_531_cov_0.632159',\n",
       " 'NODE_1271_length_520_cov_2.288939',\n",
       " 'NODE_1319_length_512_cov_2.680460',\n",
       " 'NODE_1331_length_509_cov_2.708333',\n",
       " 'NODE_1626_length_470_cov_1.249364',\n",
       " 'NODE_2056_length_427_cov_1.505714',\n",
       " 'NODE_2250_length_411_cov_3.212575',\n",
       " 'NODE_2407_length_399_cov_1.975155',\n",
       " 'NODE_2963_length_366_cov_1.515571',\n",
       " 'NODE_4040_length_319_cov_100.458678',\n",
       " 'NODE_4100_length_318_cov_0.908714',\n",
       " 'NODE_4170_length_315_cov_1.226891',\n",
       " 'NODE_4835_length_295_cov_0.944954',\n",
       " 'NODE_5538_length_277_cov_1.550000',\n",
       " 'NODE_5558_length_277_cov_0.760000',\n",
       " 'NODE_8643_length_245_cov_1.738095',\n",
       " 'NODE_9997_length_234_cov_1.076433']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hexa_contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_results = blast_results[~blast_results[\"query\"].isin(excluded_contigs[\"query\"][excluded_contigs[\"low_read_count\"]])]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_results = blast_results.reset_index().drop(columns=\"index\")\n",
    "filtered_blast_results = blast_results.groupby([\"query\"], as_index=False).apply(\n",
    "    select_taxids_for_lca, db=db,\n",
    "    return_taxid_only=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "lca_results = filtered_blast_results.groupby([\"query\"]).apply(get_lca)\n",
    "additional_hexa_contigs = lca_results[\"query\"][lca_results[\"taxid\"].apply(lambda x: ncbi.get_name_translator([\"Hexapoda\"])[\"Hexapoda\"][0] in ncbi_older_db(x, \"get_lineage\"))]\n",
    "excluded_contigs.loc[excluded_contigs[\"query\"].isin(additional_hexa_contigs), \"hexapoda\"] = True\n",
    "filtered_blast_results = filtered_blast_results[~filtered_blast_results[\"query\"].isin(additional_hexa_contigs)]\n",
    "lca_results = lca_results[~lca_results[\"query\"].isin(additional_hexa_contigs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>contig_length</th>\n",
       "      <th>read_count</th>\n",
       "      <th>low_read_count</th>\n",
       "      <th>taxid_na</th>\n",
       "      <th>hexapoda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>NODE_7_length_3154_cov_2211.364316</td>\n",
       "      <td>3154</td>\n",
       "      <td>96948.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  query  contig_length  read_count  \\\n",
       "338  NODE_7_length_3154_cov_2211.364316           3154     96948.0   \n",
       "\n",
       "     low_read_count  taxid_na  hexapoda  \n",
       "338           False     False     False  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excluded_contigs[excluded_contigs[\"query\"]==\"NODE_7_length_3154_cov_2211.364316\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
