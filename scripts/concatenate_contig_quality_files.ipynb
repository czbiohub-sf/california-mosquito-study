{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "s3_bucket = \"czbiohub-mosquito\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dirs = [x[\"Prefix\"] for x in s3.list_objects(Bucket=s3_bucket, Prefix=\"contig_quality/\", Delimiter=\"/\")[\"CommonPrefixes\"]]\n",
    "sample_dirs_rawdata = [x[\"Prefix\"] for x in s3.list_objects(Bucket=s3_bucket, Prefix=\"contigs/\", Delimiter=\"/\")[\"CommonPrefixes\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = [os.path.basename(os.path.dirname(x)) for x in sample_dirs]\n",
    "sample_names_rawdata = [os.path.basename(os.path.dirname(x)) for x in sample_dirs_rawdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sample_files = [[x[\"Key\"] for x in s3.list_objects(Bucket=s3_bucket, Prefix=y)[\"Contents\"]] for y in sample_dirs]\n",
    "\n",
    "all_sample_files_rawdata = [[x[\"Key\"] for x in s3.list_objects(Bucket=s3_bucket, Prefix=y)[\"Contents\"]] for y in sample_dirs_rawdata]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_files = [\n",
    "    \"blast_lca_nr_filtered.m9\", \n",
    "     \"blast_lca_nt_filtered.m9\",\n",
    "    \"blast_nr_filtered.m9\", \n",
    "   \"blast_nt_filtered.m9\", \n",
    "    \"lca_nr.m9\", \n",
    "   \"lca_nt.m9\", \n",
    "    \"exclude_contigs_nr.txt\", \n",
    "  \"exclude_contigs_nt.txt\",\n",
    "#    \"contig_coverage.json\",\n",
    "#    \"contig_coverage_summary.csv\",\n",
    "    \"contig_stats_all.tsv\",\n",
    "    \"contig_stats_lca.tsv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_s3_path (s3_path):\n",
    "    s3_split = os.path.normpath(s3_path).split(os.sep)\n",
    "    bucket_name = s3_split[1]\n",
    "    s3_path = '/'.join(s3_split[2:])\n",
    "    return bucket_name, s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to process blast_lca_nr_filtered.m9 files.\n",
      "read in blast_lca_nr_filtered.m9 files.\n",
      "concatenated blast_lca_nr_filtered.m9 files.\n",
      "starting to process blast_lca_nt_filtered.m9 files.\n",
      "read in blast_lca_nt_filtered.m9 files.\n",
      "concatenated blast_lca_nt_filtered.m9 files.\n",
      "starting to process blast_nr_filtered.m9 files.\n",
      "read in blast_nr_filtered.m9 files.\n",
      "concatenated blast_nr_filtered.m9 files.\n",
      "starting to process blast_nt_filtered.m9 files.\n",
      "read in blast_nt_filtered.m9 files.\n",
      "concatenated blast_nt_filtered.m9 files.\n",
      "starting to process lca_nr.m9 files.\n",
      "read in lca_nr.m9 files.\n",
      "concatenated lca_nr.m9 files.\n",
      "starting to process lca_nt.m9 files.\n",
      "read in lca_nt.m9 files.\n",
      "concatenated lca_nt.m9 files.\n",
      "starting to process exclude_contigs_nr.txt files.\n",
      "read in exclude_contigs_nr.txt files.\n",
      "concatenated exclude_contigs_nr.txt files.\n",
      "starting to process exclude_contigs_nt.txt files.\n",
      "read in exclude_contigs_nt.txt files.\n",
      "concatenated exclude_contigs_nt.txt files.\n",
      "starting to process contig_stats_all.tsv files.\n",
      "read in contig_stats_all.tsv files.\n",
      "concatenated contig_stats_all.tsv files.\n",
      "starting to process contig_stats_lca.tsv files.\n",
      "read in contig_stats_lca.tsv files.\n",
      "concatenated contig_stats_lca.tsv files.\n"
     ]
    }
   ],
   "source": [
    "for filename in summary_files:\n",
    "    print (\"starting to process \"+filename+\" files.\")\n",
    "    if (\"MosRefOnly\" in filename):\n",
    "        summary_files_names = [{sample_names[i]:\"s3://\"+s3_bucket+\"/\"+[s for s in all_sample_files[i] if os.path.basename(filename) in s and \"MosRefOnly\" in s][0]} for i, x in enumerate(sample_dirs) if ((\"MosRefOnly\" in '\\t'.join(all_sample_files[i]) and (os.path.basename(filename) in '\\t'.join(all_sample_files[i]))))]\n",
    "    elif (\"contig_coverage\" in filename):\n",
    "        summary_files_names = [{sample_names[i]:\"s3://\" + s3_bucket + \"/\" + x + filename} for i, x in enumerate(sample_dirs_rawdata) if (x + filename) in all_sample_files_rawdata[i]]\n",
    "    else:\n",
    "        summary_files_names = [{sample_names[i]:\"s3://\" + s3_bucket + \"/\" + x + filename} for i, x in enumerate(sample_dirs) if (x + filename) in all_sample_files[i]]\n",
    "    if (\"CoverageSummaryStats\" in filename):\n",
    "        summary_files_dfs = [pd.read_csv(x[list(x.keys())[0]]) for x in summary_files_names]\n",
    "    elif (\".csv\" in filename):\n",
    "        summary_files_dfs = [pd.read_csv(x[list(x.keys())[0]], header=0) for x in summary_files_names]\n",
    "        summary_files_names = [summary_files_names[i] for i, x in enumerate(summary_files_dfs) if x.columns[0] is not 'No Contigs']\n",
    "        summary_files_dfs = [x for x in summary_files_dfs if x.columns[0] is not 'No Contigs']\n",
    "    elif (any(substring in filename for substring in [\".m9\", \"exclude_contigs\", \".tsv\"])):\n",
    "        summary_files_dfs = [pd.read_csv(x[list(x.keys())[0]], sep=\"\\t\") for x in summary_files_names]\n",
    "    elif (filename.endswith(\".json\")):\n",
    "        summary_files_dfs = []\n",
    "        for fpath in summary_files_names:\n",
    "            s3_bucket_name, s3_path = split_s3_path(fpath[list(fpath.keys())[0]])\n",
    "            data_in_bytes = boto3.resource('s3').Object(s3_bucket_name, s3_path).get()[\"Body\"].read().decode('utf-8')\n",
    "            json_data = list(map(json.loads, io.StringIO(data_in_bytes).readlines()))[0]\n",
    "            outdf = pd.DataFrame(pd.Series(json_data), columns=[\"read_count\"]).reset_index(level=0).rename(columns={\"index\":\"query\"})\n",
    "            summary_files_dfs.append(outdf)\n",
    "    else:\n",
    "        summary_files_dfs = [pd.read_csv(x[list(x.keys())[0]], sep=\"\\t\", header=None) for x in summary_files_names]\n",
    "    print (\"read in \"+filename+\" files.\")\n",
    "    if ('sample' in summary_files_dfs[0].columns):\n",
    "        summary_files_df_all = pd.concat(summary_files_dfs)\n",
    "    else:\n",
    "        summary_files_df_all = pd.concat([x.assign(sample=list(summary_files_names[i].keys())[0]) for i, x in enumerate(summary_files_dfs)])\n",
    "    print (\"concatenated \"+filename+\" files.\")\n",
    "    all_dfs[filename] = summary_files_df_all        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_fs = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to upload blast_lca_nr_filtered.m9 file to s3://czbiohub-mosquito/contig_quality_concat/blast_lca_nr_filtered.m9\n",
      "finished uploading blast_lca_nr_filtered.m9 files.\n",
      "starting to upload blast_lca_nt_filtered.m9 file to s3://czbiohub-mosquito/contig_quality_concat/blast_lca_nt_filtered.m9\n",
      "finished uploading blast_lca_nt_filtered.m9 files.\n",
      "starting to upload blast_nr_filtered.m9 file to s3://czbiohub-mosquito/contig_quality_concat/blast_nr_filtered.m9\n",
      "finished uploading blast_nr_filtered.m9 files.\n",
      "starting to upload blast_nt_filtered.m9 file to s3://czbiohub-mosquito/contig_quality_concat/blast_nt_filtered.m9\n",
      "finished uploading blast_nt_filtered.m9 files.\n",
      "starting to upload lca_nr.m9 file to s3://czbiohub-mosquito/contig_quality_concat/lca_nr.m9\n",
      "finished uploading lca_nr.m9 files.\n",
      "starting to upload lca_nt.m9 file to s3://czbiohub-mosquito/contig_quality_concat/lca_nt.m9\n",
      "finished uploading lca_nt.m9 files.\n",
      "starting to upload exclude_contigs_nr.txt file to s3://czbiohub-mosquito/contig_quality_concat/exclude_contigs_nr.txt\n",
      "finished uploading exclude_contigs_nr.txt files.\n",
      "starting to upload exclude_contigs_nt.txt file to s3://czbiohub-mosquito/contig_quality_concat/exclude_contigs_nt.txt\n",
      "finished uploading exclude_contigs_nt.txt files.\n",
      "starting to upload contig_stats_all.tsv file to s3://czbiohub-mosquito/contig_quality_concat/contig_stats_all.tsv\n",
      "finished uploading contig_stats_all.tsv files.\n",
      "starting to upload contig_stats_lca.tsv file to s3://czbiohub-mosquito/contig_quality_concat/contig_stats_lca.tsv\n",
      "finished uploading contig_stats_lca.tsv files.\n"
     ]
    }
   ],
   "source": [
    "for filename in summary_files:\n",
    "    if filename==\"contig_coverage.json\":\n",
    "        filename =  \"contig_coverage.tsv\"\n",
    "    upload_fn = s3_bucket+'/contig_quality_concat/'+filename\n",
    "    print (\"starting to upload \"+filename+\" file to s3://\"+upload_fn)\n",
    "    with s3_fs.open(upload_fn,'w') as f:\n",
    "        if (\"CoverageSummaryStats\" in filename):\n",
    "            all_dfs[filename].to_csv(f, index=False)\n",
    "        elif (\".csv\" in filename):\n",
    "            all_dfs[filename].to_csv(f, sep=\",\", header=True, index=False)\n",
    "        elif (any(substring in filename for substring in [\".m9\", \"exclude_contigs\", \"contig_coverage\", \".tsv\"])):\n",
    "            all_dfs[filename].to_csv(f, sep=\"\\t\", index=False)\n",
    "        else:\n",
    "            all_dfs[filename].to_csv(f, sep=\"\\t\", header=False, index=False)\n",
    "    print (\"finished uploading \"+filename+\" files.\")  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
