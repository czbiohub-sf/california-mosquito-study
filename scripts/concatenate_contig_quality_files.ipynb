{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import io\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "s3_bucket = \"czbiohub-mosquito\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dirs = [x[\"Prefix\"] for x in s3.list_objects(Bucket=s3_bucket, Prefix=\"contig_quality/\", Delimiter=\"/\")[\"CommonPrefixes\"]]\n",
    "sample_dirs_rawdata = [x[\"Prefix\"] for x in s3.list_objects(Bucket=s3_bucket, Prefix=\"contigs/\", Delimiter=\"/\")[\"CommonPrefixes\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_names = [os.path.basename(os.path.dirname(x)) for x in sample_dirs]\n",
    "sample_names_rawdata = [os.path.basename(os.path.dirname(x)) for x in sample_dirs_rawdata]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sample_files = [[x[\"Key\"] for x in s3.list_objects(Bucket=s3_bucket, Prefix=y)[\"Contents\"]] for y in sample_dirs]\n",
    "\n",
    "all_sample_files_rawdata = [[x[\"Key\"] for x in s3.list_objects(Bucket=s3_bucket, Prefix=y)[\"Contents\"]] for y in sample_dirs_rawdata]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sample_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sample_files_rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_files = [\n",
    "    \"blast_lca_nr_filtered.m9\", \n",
    "    \"blast_lca_nt_filtered.m9\",\n",
    "    \"blast_nr_filtered.m9\", \n",
    "    \"blast_nt_filtered.m9\", \n",
    "    \"lca_nr.m9\", \n",
    "    \"lca_nt.m9\", \n",
    "    \"exclude_contigs_nr.txt\", \n",
    "    \"exclude_contigs_nt.txt\",\n",
    "    \"contig_coverage.json\",\n",
    "    \"contig_coverage_summary.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_s3_path (s3_path):\n",
    "    s3_split = os.path.normpath(s3_path).split(os.sep)\n",
    "    bucket_name = s3_split[1]\n",
    "    s3_path = '/'.join(s3_split[2:])\n",
    "    return bucket_name, s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in summary_files:\n",
    "    print (\"starting to process \"+filename+\" files.\")\n",
    "    if (\"MosRefOnly\" in filename):\n",
    "        summary_files_names = [{sample_names[i]:\"s3://\"+s3_bucket+\"/\"+[s for s in all_sample_files[i] if os.path.basename(filename) in s and \"MosRefOnly\" in s][0]} for i, x in enumerate(sample_dirs) if ((\"MosRefOnly\" in '\\t'.join(all_sample_files[i]) and (os.path.basename(filename) in '\\t'.join(all_sample_files[i]))))]\n",
    "    elif (\"contig_coverage\" in filename):\n",
    "        summary_files_names = [{sample_names[i]:\"s3://\" + s3_bucket + \"/\" + x + filename} for i, x in enumerate(sample_dirs_rawdata) if (x + filename) in all_sample_files_rawdata[i]]\n",
    "    else:\n",
    "        summary_files_names = [{sample_names[i]:\"s3://\" + s3_bucket + \"/\" + x + filename} for i, x in enumerate(sample_dirs) if (x + filename) in all_sample_files[i]]\n",
    "    if (\"CoverageSummaryStats\" in filename):\n",
    "        summary_files_dfs = [pd.read_csv(x[list(x.keys())[0]]) for x in summary_files_names]\n",
    "    elif (\".csv\" in filename):\n",
    "        summary_files_dfs = [pd.read_csv(x[list(x.keys())[0]], header=0) for x in summary_files_names]\n",
    "        summary_files_names = [summary_files_names[i] for i, x in enumerate(summary_files_dfs) if x.columns[0] is not 'No Contigs']\n",
    "        summary_files_dfs = [x for x in summary_files_dfs if x.columns[0] is not 'No Contigs']\n",
    "    elif (any(substring in filename for substring in [\".m9\", \"exclude_contigs\"])):\n",
    "        summary_files_dfs = [pd.read_csv(x[list(x.keys())[0]], sep=\"\\t\") for x in summary_files_names]\n",
    "    elif (filename.endswith(\".json\")):\n",
    "        summary_files_dfs = []\n",
    "        for fpath in summary_files_names:\n",
    "            s3_bucket_name, s3_path = split_s3_path(fpath[list(fpath.keys())[0]])\n",
    "            data_in_bytes = boto3.resource('s3').Object(s3_bucket_name, s3_path).get()[\"Body\"].read().decode('utf-8')\n",
    "            json_data = list(map(json.loads, io.StringIO(data_in_bytes).readlines()))[0]\n",
    "            outdf = pd.DataFrame(pd.Series(json_data), columns=[\"read_count\"]).reset_index(level=0).rename(columns={\"index\":\"query\"})\n",
    "            summary_files_dfs.append(outdf)\n",
    "    else:\n",
    "        summary_files_dfs = [pd.read_csv(x[list(x.keys())[0]], sep=\"\\t\", header=None) for x in summary_files_names]\n",
    "    print (\"read in \"+filename+\" files.\")\n",
    "    if ('sample' in summary_files_dfs[0].columns):\n",
    "        summary_files_df_all = pd.concat(summary_files_dfs)\n",
    "    else:\n",
    "        summary_files_df_all = pd.concat([x.assign(sample=list(summary_files_names[i].keys())[0]) for i, x in enumerate(summary_files_dfs)])\n",
    "    print (\"concatenated \"+filename+\" files.\")\n",
    "    all_dfs[filename] = summary_files_df_all        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_fs = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in summary_files:\n",
    "    if filename==\"contig_coverage.json\":\n",
    "        filename =  \"contig_coverage.tsv\"\n",
    "    upload_fn = s3_bucket+'/contig_quality_concat/'+filename\n",
    "    print (\"starting to upload \"+filename+\" file to s3://\"+upload_fn)\n",
    "    with s3_fs.open(upload_fn,'w') as f:\n",
    "        if (\"CoverageSummaryStats\" in filename):\n",
    "            all_dfs[filename].to_csv(f, index=False)\n",
    "        elif (\".csv\" in filename):\n",
    "            all_dfs[filename].to_csv(f, sep=\",\", header=True, index=False)\n",
    "        elif (any(substring in filename for substring in [\".m9\", \"exclude_contigs\", \"contig_coverage\"])):\n",
    "            all_dfs[filename].to_csv(f, sep=\"\\t\", index=False)\n",
    "        else:\n",
    "            all_dfs[filename].to_csv(f, sep=\"\\t\", header=False, index=False)\n",
    "    print (\"finished uploading \"+filename+\" files.\")  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csp_counts = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/MosRefOnly/bowtie_csp_counts_1000.txt\", sep=\"\\t\", header=None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csp_counts[csp_counts[0].str.contains(\"_1_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"MosRefOnly/bowtie_csp_counts_1000.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_files_names = [{sample_names[i]:\"s3://\"+s3_bucket+\"/\"+[s for s in all_sample_files[i] if os.path.basename(filename) in s and \"MosRefOnly\" in s][0]} for i, x in enumerate(sample_dirs) if ((\"MosRefOnly\" in '\\t'.join(all_sample_files[i]) and (os.path.basename(filename) in '\\t'.join(all_sample_files[i]))))]\n",
    "summary_files_dfs = [pd.read_csv(x[list(x.keys())[0]], sep=\"\\t\", header=None) for x in summary_files_names]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "summary_files_dfs[i][summary_files_dfs[i][0].str.contains(\"_1_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, x in enumerate(sample_dirs):\n",
    "    if (\"MosRefOnly\" in '\\t'.join(all_sample_files[i])):\n",
    "       print (i)\n",
    "       {sample_names[i]:\"s3://\"+s3_bucket+\"/\"+[s for s in all_sample_files[i] if os.path.basename(filename) in s and \"MosRefOnly\" in s][0]}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=61\n",
    "all_sample_files[i]\n",
    "#[s for s in all_sample_files[i] if os.path.basename(filename) in s and \"MosRefOnly\" in s]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_files_df_all = pd.concat([x.assign(sample=list(summary_files_names[i].keys())[0]) for i, x in enumerate(summary_files_dfs)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_lca = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality_concat/blast_lca_nt_filtered.m9\", sep=\"\\t\", header=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_lca[(blast_lca[\"taxid\"]==1) & (blast_lca[\"query\"].str.split(\"_\").apply(lambda x: int(x[3]))>1000)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(blast_lca[\"taxid\"]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_results = pd.read_csv(\"s3://czbiohub-mosquito/contig_quality/CMS001_009_Ra_S13/blast_nt_filtered.m9\", sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_results[blast_results[\"query\"]==\"NODE_1_length_10423_cov_75.085830\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
